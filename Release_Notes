				                       Release Notes

TORQUE Resource Manager

Software Version 2.5.0

This file contains a few notes on building and installing TORQUE and 
Release notes concerning some of the new features of this release.

		                   	-------- WARNING! --------

TORQUE 2.5 Array Changes - WARNING: TORQUE 2.5 Job Arrays are not backwards
compatible, you MUST NOT upgrade to this from 2.3 or 2.4 while you have job
arrays queued - See README.array_changes for more details.

---------------------------------------------------------------------------
CONTENTS

   This document contains the following sections:
     
     * Overview
     * What's New
     * Requirements
     * Installation

--------------------------------------------------------------------------
OVERVIEW

    Enhanced job array functionality and Cygwin compatibility are the two
    main new features of TORQUE 2.5.0. Several other minor enhancements 
    have also been added to TORQUE 2.5.0 along with several bug fixes.
    For a complete list of changes see CHANGELOG.
    
---------------------------------------------------------------------------
WHAT'S NEW

New in 2.5.4
    Generic GPGPU support has been added to TORQUE 2.5.4. Users are able to
    manually set the number of GPUs on a system in the $TORQUE_HOME/server_priv/nodes
    file. GPUs are then requested on job submission as part of the nodes resource
    list using the following syntax:  -l nodes=X[:ppn=Y][:gpus=Z]. The allocated
    gpus appear in $PBS_GPUFILE, a new environment variable, in the 
    form: <hostname>-gpu<index> and in a  new job attribute exec_gpus:
    <hostname>-gpu/<index>[+<hostname>-gpu/<index>...]. For more information
    about using GPUs with TORQUE 2.5.4 see the documentation in section 1.5.3 at 
    http://www.clusterresources.com/products/torque/docs/1.5nodeconfig.shtml
    and also section 2.1.2 at 
    http://www.clusterresources.com/products/torque/docs/2.1jobsubmission.shtml#resources

    The buildutils/torque.spec.in file has been modified to comply more closely with RPM
    standards. Users may experience some unexpected behavior from what they have experienced
    in past versions of TORQUE. Please post your questions, problems and concerns to
    the mailing list at torqueuers@supercluster.org

    While Adaptive Computing distributes the RPM files as part of the build it does not support 
    those files. Not every Linux distribution uses RPM. Adaptive Computing provides a single
    solution using make and make install that works across all Linux distributions and most
    UNIX systems. We recognize the RPM format provides many advantages for deployment but 
    it is up to the indiviual site to repackage the TORQUE installation to match their individual
    needs.

    If you have issues with the new spec files please post the issue or questions to the torque users
    (torqueusers@supercluster.org) mailing list


New in 2.5.3
    Completed job information can now be logged. A new Boolean server parameter 
    record_job_info can be set to TRUE and a log file will be created
    under $TORQUE_HOME/job_logs. The log file is in XML format and 
    contains the same information that would be produced by qstat -f.
    For more information about how to setup and use job logs go to
    http://www.clusterresources.com/products/torque/docs/10.1joblogging.shtml
    

    The serverdb file which contains the queue and server configuration
    data can optionally be converted to XML format. If you configure 
    TORQUE with the --enable-server-xml option the serverdb file will
    be stored in XML format. If you are upgrading from a version of 
    TORQUE earlier than 2.5.3 the old serverdb file will be converted 
    to the new XML format.
    
    WARNING -- If you wish to upgrade to the XML serverdb format please
    backup the serverdb before doing the upgrade. This is a one-way upgrade.
    Once the file has been converted to XML it cannot be converted back
    to the binary format.

    Munge has been added as an option for user authorization on the server.
    The default user authorization for TORQUE uses privileged ports and 
    ruserok to authorize client applications. Munge creates an alternative 
    which is more scalable and can bypass the rsh type ruserok function
    call. For more information see 1.3.2.8 Using MUNGE Authentication at
    http://www.clusterresources.com/products/torque/docs/1.3advconfig.shtml

New in verions 2.5.2 and earlier 2.5.x versions.

    Job arrays are now suported in the commands qalter, qdel, qhold, qrls 
    in addition to the qsub command.

    Slot limits are a new feature added to job arrays which allow users and
    administrators to have more control of the number of concurrently 
    running jobs from a job array. Slot limits can be set on a per job basis
    or system wide with the new server parameter 'max_slot_limit'. 
    Administrators can also control how large user arrays can be with
    the new server parameter 'max_job_array_size'.

    New job dependecy options have been added to work with job arrays. Users
    can create dependecies based on the status of entire job arrays and
    not just individual jobs.

    qstat has also been updated to more conveniently display job array. Job
    arrays are displayed in a summary of the array by default, however,
    expanded display of the entire job can also be done.

    Special thanks to Glen Beane and David Beer for their work on the 
    new job array functionality. For more information concerning updates 
    to job arrays in TORQUE 2.5.0 refer to the README.array_changes document.

    TORQUE 2.5.0 can now be run with Cygwin. This feature was added by
    Igor Ilyenko, Yauheni Charniauski and Vikentsi Lapa. To learn how to 
    run TORQUE with Cygwin see README.cygwin. TORQUE on Cygwin was a
    community project and support for this feature will be provided by
    the TORQUE community. 
    
    For more information concerning the installation and use of TORQUE
    with Cygwin please see the README.cygwin file.

    The 'procs' keyword has been part of the qsub syntax for some time. 
    However, TORQUE itself never interpreted this argument and simply 
    passed it through to the scheduler. With TORQUE 2.5.0 the 'procs' 
    keyword is now interpreted to mean allocate a designated number of 
    processors on any combination of nodes. For example the following
    qsub command

    qsub -l nodes=2 -l procs=2 

    will allocate two separate nodes with one processor each plus it will
    allocate two additional processors from any other available nodes.
    The same allocation can be achieved with the following syntax as well.

    qsub -l nodes=2+procs=2.

    A new MOM config option was added named 'alias_server_name'. This option
    allows a MOM to add an additional host name address to its trusted 
    addresses. The option was added to overcome a problem with RPP and UDP
    when alias IP addresses are used on a pbs_server.

    'clone_batch_size', 'clone_batch_delay', 'job_start_timeout', and 
    'checkpoint_defaults' were added as new qmgr server parameters. 

    To find more information concerning the new parameters as well as other
    TORQUE features see the documentation at 
    http://www.clusterresources.com/products/torque/docs/




REQUIREMENTS
------------

An ANSI C compiler is required.   The native C compiler is recommended if it
is ANSI, otherwise use gcc.

A fully POSIX make is required.  If you are unable to "make" PBS with your
make, we suggest use of gmake from GNU.

Tcl/Tk version 8 or higher is required if you plan to build the GUI portion
of TORQUE or use a Tcl based scheduler.


BUILD AND INSTALLATION DIRECTIONS
---------------------------------

The directions to build and install are found in the PBS Administrators Guide.
A postscript and PDF copy are found in this directory.  Please read and
follow the directions CAREFULLY.

Installation instructions can also be found at http://www.clusterresources.com/products/torque/docs/


