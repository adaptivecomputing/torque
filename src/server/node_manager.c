/*
*         OpenPBS (Portable Batch System) v2.3 Software License
*
* Copyright (c) 1999-2000 Veridian Information Solutions, Inc.
* All rights reserved.
*
* ---------------------------------------------------------------------------
* For a license to use or redistribute the OpenPBS software under conditions
* other than those described below, or to purchase support for this software,
* please contact Veridian Systems, PBS Products Department ("Licensor") at:
*
*    www.OpenPBS.org  +1 650 967-4675                  sales@OpenPBS.org
*                        877 902-4PBS (US toll-free)
* ---------------------------------------------------------------------------
*
* This license covers use of the OpenPBS v2.3 software (the "Software") at
* your site or location, and, for certain users, redistribution of the
* Software to other sites and locations.  Use and redistribution of
* OpenPBS v2.3 in source and binary forms, with or without modification,
* are permitted provided that all of the following conditions are met.
* After December 31, 2001, only conditions 3-6 must be met:
*
* 1. Commercial and/or non-commercial use of the Software is permitted
*    provided a current software registration is on file at www.OpenPBS.org.
*    If use of this software contributes to a publication, product, or
*    service, proper attribution must be given; see www.OpenPBS.org/credit.html
*
* 2. Redistribution in any form is only permitted for non-commercial,
*    non-profit purposes.  There can be no charge for the Software or any
*    software incorporating the Software.  Further, there can be no
*    expectation of revenue generated as a consequence of redistributing
*    the Software.
*
* 3. Any Redistribution of source code must retain the above copyright notice
*    and the acknowledgment contained in paragraph 6, this list of conditions
*    and the disclaimer contained in paragraph 7.
*
* 4. Any Redistribution in binary form must reproduce the above copyright
*    notice and the acknowledgment contained in paragraph 6, this list of
*    conditions and the disclaimer contained in paragraph 7 in the
*    documentation and/or other materials provided with the distribution.
*
* 5. Redistributions in any form must be accompanied by information on how to
*    obtain complete source code for the OpenPBS software and any
*    modifications and/or additions to the OpenPBS software.  The source code
*    must either be included in the distribution or be available for no more
*    than the cost of distribution plus a nominal fee, and all modifications
*    and additions to the Software must be freely redistributable by any party
*    (including Licensor) without restriction.
*
* 6. All advertising materials mentioning features or use of the Software must
*    display the following acknowledgment:
*
*     "This product includes software developed by NASA Ames Research Center,
*     Lawrence Livermore National Laboratory, and Veridian Information
*     Solutions, Inc.
*     Visit www.OpenPBS.org for OpenPBS software support,
*     products, and information."
*
* 7. DISCLAIMER OF WARRANTY
*
* THIS SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND. ANY EXPRESS
* OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
* OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT
* ARE EXPRESSLY DISCLAIMED.
*
* IN NO EVENT SHALL VERIDIAN CORPORATION, ITS AFFILIATED COMPANIES, OR THE
* U.S. GOVERNMENT OR ANY OF ITS AGENCIES BE LIABLE FOR ANY DIRECT OR INDIRECT,
* INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
* LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
* OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
* LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
* NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
* EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*
* This license will be governed by the laws of the Commonwealth of Virginia,
* without reference to its choice of law rules.
*/

#include <pbs_config.h>   /* the master config generated by configure */
#include "node_manager.h"

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <ctype.h>
#include <errno.h>
#include <sys/types.h>
#include <netinet/in.h>
#include <stdarg.h>
#include <assert.h>
#if defined(NTOHL_NEEDS_ARPA_INET_H) && defined(HAVE_ARPA_INET_H)
#include <arpa/inet.h>
#endif


#include "portability.h"
#include "libpbs.h"
#include "server_limits.h"
#include "list_link.h"
#include "attribute.h"
#include "resource.h"
#include "server.h"
#include "net_connect.h"
#include "batch_request.h"
#include "work_task.h"
#include "svrfunc.h"
#include "pbs_job.h"
#include "log.h"
#include "../lib/Liblog/pbs_log.h"
#include "../lib/Liblog/log_event.h"
#include "pbs_nodes.h"
#include "rpp.h"
#include "dis.h"
#include "dis_init.h"
#include "resmon.h"
#include "mcom.h"
#include "utils.h"
#include "u_tree.h"
#include "threadpool.h"
#include "node_func.h" /* find_nodebyname */
#include "../lib/Libutils/u_lock_ctl.h" /* lock_node, unlock_node */
#include "../lib/Libnet/lib_net.h" /* socket_read_flush */
#include "svr_func.h" /* get_svr_attr_* */
#include "alps_functions.h"
#include "login_nodes.h"
#include "svr_connect.h" /* svr_disconnect_sock */
#include "net_cache.h"
#include "ji_mutex.h"
#include "alps_constants.h"

#define IS_VALID_STR(STR)  (((STR) != NULL) && ((STR)[0] != '\0'))
#define SEND_HELLO 11

extern int              LOGLEVEL;

extern int              allow_any_mom;

#if !defined(H_ERRNO_DECLARED) && !defined(_AIX)
extern int              h_errno;
#endif
  
const char              shared[] = "shared";

int                     svr_totnodes = 0; /* total number nodes defined       */
int                     svr_clnodes  = 0; /* number of cluster nodes     */
int                     svr_tsnodes  = 0; /* number of time shared nodes     */
int                     svr_chngNodesfile = 0; /* 1 signals want nodes file update */
int                     gpu_mode_rqstd = -1;  /* default gpu mode requested */
#ifdef NVIDIA_GPUS
int                     gpu_err_reset = FALSE;    /* was a gpu errcount reset requested */
#endif  /* NVIDIA_GPUS */
/* on server shutdown, (qmgr mods)  */

all_nodes               allnodes;

static int              exclusive;  /* node allocation type */

static int              num_addrnote_tasks = 0; /* number of outstanding send_cluster_addrs tasks */
pthread_mutex_t        *addrnote_mutex = NULL;

extern int              server_init_type;
extern int              has_nodes;

#ifdef NVIDIA_GPUS
extern int create_a_gpusubnode(struct pbsnode *);
int        is_gpustat_get(struct pbsnode *np, char **str_ptr);
#endif  /* NVIDIA_GPUS */

extern int              ctnodes(char *);

extern char            *path_home;
extern char            *path_nodes;
extern char            *path_nodes_new;
extern char            *path_nodestate;
extern char            *path_nodenote;
extern char            *path_nodenote_new;
extern char             server_name[];

extern struct server    server;
extern tlist_head       svr_newnodes;
extern attribute_def    node_attr_def[];   /* node attributes defs */
extern int              SvrNodeCt;
extern hello_container  hellos;
extern struct all_jobs  alljobs;

extern int              multi_mom;

#define SKIP_NONE       0
#define SKIP_EXCLUSIVE  1
#define SKIP_ANYINUSE   2
#define SKIP_NONE_REUSE 3

#ifndef MAX_BM
#define MAX_BM          64
#endif

int handle_complete_first_time(job *pjob);
int is_compute_node(char *node_id);
int hasprop(struct pbsnode *, struct prop *);
int add_job_to_node(struct pbsnode *,struct pbssubn *,short,job *,int);
int node_satisfies_request(struct pbsnode *,char *);
int reserve_node(struct pbsnode *,short,job *,char *,struct howl **);
int build_host_list(struct howl **,struct pbssubn *,struct pbsnode *);
int procs_available(int proc_ct);
void check_nodes(struct work_task *);
#ifdef NVIDIA_GPUS
int gpu_entry_by_id(struct pbsnode *,char *, int);
#endif  /* NVIDIA_GPUS */
job *get_job_from_jobinfo(struct jobinfo *,struct pbsnode *);

/* marks a stream as finished being serviced */
pthread_mutex_t        *node_state_mutex = NULL;



typedef struct sync_job_info
  {
  char   *input;
  time_t  timestamp;
  } sync_job_info;



/**
**      Modified by Tom Proett <proett@nas.nasa.gov> for PBS.
*/

AvlTree                 ipaddrs = NULL;


/**
 * specialized version of tfind for looking in the ipadders tree
 * @param key - the node we are searching for
 * @return a pointer to the pbsnode
*/

struct pbsnode *tfind_addr(

  const u_long  key,
  uint16_t      port,
  char         *job_momname)

  {
  struct pbsnode *pn = AVL_find(key,port,ipaddrs);

  if (pn == NULL)
    return(NULL);

  lock_node(pn, __func__, "pn", LOGLEVEL);

  if (pn->num_node_boards == 0)
    return(pn);
  else
    {
    char *dash = NULL;
    char *plus = NULL;
    char *tmp = job_momname;

    struct pbsnode *numa = NULL;

    int index;

    plus = strchr(tmp,'+');

    if (plus != NULL)
      *plus = '\0';

    while ((tmp = strchr(tmp,'-')) != NULL)
      {
      dash = tmp;
      tmp++;
      }

    if (dash == NULL)
      {
      /* node has numa nodes but no dashes in exec host?? */
      log_err(-1, __func__, "Numa node but there's no dash in exec_host?");

      return(pn);
      }

    index = atoi(dash+1);

    numa = AVL_find(index, pn->nd_mom_port, pn->node_boards);

    unlock_node(pn, __func__, "pn->numa", LOGLEVEL);
    lock_node(numa, __func__, "numa", LOGLEVEL);

    if (plus != NULL)
      *plus = '+';

    return(numa);
    }
  } /* END tfind_addr() */




/* update_node_state - central location for updating node state */
/* NOTE:  called each time a node is marked down, each time a MOM reports node  */
/*        status, and when pbs_server sends hello/cluster_addrs */

void update_node_state(

  struct pbsnode *np,         /* I (modified) */
  int             newstate)   /* I (one of INUSE_*) */

  {
  char            log_buf[LOCAL_LOG_BUF_SIZE];

  struct pbssubn *sp;

  /*
   * LOGLEVEL >= 4 logs all state changes
   *          >= 2 logs down->(busy|free) changes
   *          (busy|free)->down changes are always logged
   */

  if (LOGLEVEL >= 3)
    {
    sprintf(log_buf, "adjusting state for node %s - state=%d, newstate=%d",
      (np->nd_name != NULL) ? np->nd_name : "NULL",
      np->nd_state,
      newstate);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  log_buf[0] = '\0';

  if (newstate & INUSE_DOWN)
    {
    if (!(np->nd_state & INUSE_DOWN))
      {
      sprintf(log_buf, "node %s marked down",
        (np->nd_name != NULL) ? np->nd_name : "NULL");

      np->nd_state |= INUSE_DOWN;
      np->nd_state &= ~INUSE_UNKNOWN;

      /* mark all subnodes down */

      for (sp = np->nd_psn;sp != NULL;sp = sp->next)
        {
        sp->inuse |= INUSE_DOWN;
        }
      }

    /* ignoring the obvious possibility of a "down,busy" node */
    }  /* END if (newstate & INUSE_DOWN) */
  else if (newstate & INUSE_BUSY)
    {
    if ((!(np->nd_state & INUSE_BUSY) && (LOGLEVEL >= 4)) ||
        ((np->nd_state & INUSE_DOWN) && (LOGLEVEL >= 2)))
      {
      sprintf(log_buf, "node %s marked busy",
        (np->nd_name != NULL) ? np->nd_name : "NULL");
      }

    np->nd_state |= INUSE_BUSY;

    np->nd_state &= ~INUSE_UNKNOWN;

    if (np->nd_state & INUSE_DOWN)
      {
      np->nd_state &= ~INUSE_DOWN;

      /* clear down on all subnodes */

      for (sp = np->nd_psn;sp != NULL;sp = sp->next)
        {
        sp->inuse &= ~INUSE_DOWN;
        }
      }
    }  /* END else if (newstate & INUSE_BUSY) */
  else if (newstate == INUSE_FREE)
    {
    if (((np->nd_state & INUSE_DOWN) && (LOGLEVEL >= 2)) ||
        ((np->nd_state & INUSE_BUSY) && (LOGLEVEL >= 4)))
      {
      sprintf(log_buf, "node %s marked free",
        (np->nd_name != NULL) ? np->nd_name : "NULL");
      }

    np->nd_state &= ~INUSE_BUSY;

    np->nd_state &= ~INUSE_UNKNOWN;

#ifdef BROKENVNODECHECKS

    if ((np->nd_state & INUSE_JOB) ||
        (np->nd_state & INUSE_JOBSHARE) ||
        (np->nd_nsn != np->nd_nsnfree))
      {
      int snjcount;   /* total number of jobs assigned to nodes */
      int snjacount;  /* number of subnodes with job array associated with them */

      int nsn_free;
      int SNIsAllocated;  /* boolean */

      struct jobinfo *jp;

      struct jobinfo *jpprev;

      char   tmpLine[1024];

      /* count jobs on all subnodes */

      snjcount = 0;
      snjacount = 0;

      /* initially set free subnode count to config subnode count */

      nsn_free = np->nd_nsn;

      for (sp = np->nd_psn;sp != NULL;sp = sp->next)
        {
        if (sp->jobs != NULL)
          {
          SNIsAllocated = 0;  /* mark subnode allocated only after job detected */

          snjacount++;

          sp->inuse &= ~(INUSE_JOB | INUSE_JOBSHARE);

          /* look for and remove duplicate job entries in subnode job list */

          jpprev = NULL;

          for (jp = sp->jobs;jp != NULL;jp = jp->next)
            {
            if (jp->job != NULL)
              {
              if ((jp->job->ji_qs.ji_state != JOB_STATE_RUNNING) ||
                  (jp->job->ji_qs.ji_substate == JOB_SUBSTATE_SUSPEND) ||
                  (jp->job->ji_wattr[JOB_ATR_state].at_val.at_char == 'S'))
                {
                /* only count suspended and running jobs */

                continue;
                }

              snjcount++;

              if (SNIsAllocated == 0)
                {
                SNIsAllocated = 1;

                nsn_free--;
                }
              }

            if ((jpprev != NULL) && (jpprev->job == jp->job))
              {
              /* duplicate job entry detected */
              sprintf(tmpLine, "ALERT:  duplicate entry for job '%s' detected on node %s (clearing entry)",
                (jp->job != NULL) ? jp->job->ji_qs.ji_jobid : "???",
                np->nd_name);

              log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, tmpLine);

              jpprev->next = jp->next;

              free(jp);

              break;
              }

            jpprev = jp;
            }  /* END for (jp) */
          }    /* END if (sp->jobs != NULL) */
        }      /* END for (sp) */

      if (snjcount == 0)
        {
        /* node has no jobs but is in allocated state - free subnodes */

        np->nd_nsnfree = np->nd_nsn;

        sprintf(log_buf, "job allocation released on node %s - node marked free",
          (np->nd_name != NULL) ? np->nd_name : "NULL");

        if (snjacount > 0)
          {
          strcat(log_buf, " - subnode job array is corrupt");
          }

        np->nd_state &= ~(INUSE_JOB | INUSE_JOBSHARE);
        }
      else
        {
        if (np->nd_nsnfree != nsn_free)
          {
          sprintf(log_buf, "subnode allocation adjusted on node %s (%d -> %d)",
            (np->nd_name != NULL) ? np->nd_name : "NULL",
            np->nd_nsnfree,
            nsn_free);

          np->nd_nsnfree = nsn_free;

          /* what is the exact meaning of JOBSHARE? */

          np->nd_state &= ~INUSE_JOBSHARE;
          }
        else
          {
          /* subnode availability values are correct */

          if (LOGLEVEL >= 7)
            {
            sprintf(log_buf, "subnode allocation correct on node %s (%d free, %d configured)",
              (np->nd_name != NULL) ? np->nd_name : "NULL",
              np->nd_nsnfree,
              np->nd_nsn);
            }
          }

        if (np->nd_nsnfree > 0)
          {
          /* if any sub-nodes are free, job cannot be in job-exclusive */

          np->nd_state &= ~INUSE_JOB;

          if (LOGLEVEL >= 3)
            {
            if (log_buf[0] == '\0')
              sprintf(log_buf,
                "unset job-exclusive state for node %s in state %d (%d free, %d configured)",
                (np->nd_name != NULL) ? np->nd_name : "NULL",
                np->nd_state,
                np->nd_nsnfree,
                np->nd_nsn);
            else
              strcat(log_buf, "(unset job-exclusive state)");
            }
          }
        }    /* END else (snjcount == 0) */
      }      /* END if ((np->nd_state & INUSE_JOB) || ...) */
    else
      {
      /* skipping subnode allocation check */

      if (LOGLEVEL >= 7)
        {
        sprintf(log_buf,
          "skipping subnode allocation test for node %s in state %d (%d free, %d configured)\n",
          (np->nd_name != NULL) ? np->nd_name : "NULL",
          np->nd_state,
          np->nd_nsnfree,
          np->nd_nsn);
        }
      }

#endif /* BROKENVNODECHECKS */

    if (np->nd_state & INUSE_DOWN)
      {
      np->nd_state &= ~INUSE_DOWN;

      /* clear down on all subnodes */

      for (sp = np->nd_psn;sp != NULL;sp = sp->next)
        {
        sp->inuse &= ~INUSE_DOWN;
        }
      }
    }    /* END else if (newstate == INUSE_FREE) */

  if (newstate & INUSE_UNKNOWN)
    {
    np->nd_state |= INUSE_UNKNOWN;
    }

  if ((LOGLEVEL >= 2) && (log_buf[0] != '\0'))
    {
    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  return;
  }  /* END update_node_state() */





int check_node_for_job(

  struct pbsnode *pnode,
  char           *jobid)

  {
  struct pbssubn *np;
  struct jobinfo *jp;

  /* just check each subnode for the job */
  for (np = pnode->nd_psn;np != NULL;np = np->next)
    {
    /* for each jobinfo on subnode on node ... */
    for (jp = np->jobs; jp != NULL; jp = jp->next)
      {
      if (strcmp(jobid, jp->jobid) == 0)
        {
        return(TRUE);
        }
      } /* END for each job on the subnode */
    } /* END for each subnode */

  /* not found */
  return(FALSE);
  } /* END check_node_for_job() */




/*
 * is_job_on_node - return TRUE if this jobid is present on pnode
 */

int is_job_on_node(

  struct pbsnode *pnode, /* I */
  char           *jobid) /* I */

  {
  struct pbsnode *numa;

  int             present = FALSE;
  char           *at;
  int             i;

  if ((at = strchr(jobid, (int)'@')) != NULL)
    *at = '\0'; /* strip off @server_name */

  if (pnode->num_node_boards > 0)
    {
    /* check each subnode on each numa node for the job */
    for (i = 0; i < pnode->num_node_boards; i++)
      {
      numa = AVL_find(i,pnode->nd_mom_port,pnode->node_boards);

      lock_node(numa, __func__, "before check_node_for_job numa", LOGLEVEL);
      present = check_node_for_job(pnode,jobid);
      unlock_node(numa, __func__, "after check_node_for_job numa", LOGLEVEL);

      /* leave loop if we found the job */
      if (present != FALSE)
        break;
      } /* END for each numa node */
    }
  else
    {
    present = check_node_for_job(pnode, jobid);
    }

  if (at != NULL)
    *at = '@';  /* restore @server_name */

  return(present);
  }  /* END is_job_on_node() */



/* If nodes have similiar names this will make sure the name is an exact match.
 * Not just found inside another name.
 * i.e. Machines by the name of gpu, gpuati, gpunvidia. If searching for gpu...
 * List format is similiar to: gpuati+gpu/1+gpunvidia/4+gpu/5
 */
int node_in_exechostlist(
    
  char *node_name,
  char *node_ehl)

  {
  int   rc = FALSE;
  char *cur_pos = node_ehl;
  char *new_pos = cur_pos;
  int   name_len = strlen(node_name);

  while (1)
    {
    if ((new_pos = strstr(cur_pos, node_name)) == NULL)
      break;
    else if (new_pos == node_ehl)
      {
      if ((new_pos+name_len == NULL) ||
          (*(new_pos+name_len) == '+') ||
          (*(new_pos+name_len) == '/'))
        {
        rc = TRUE;
        break;
        }
      }
    else if (*(new_pos-1) == '+')
      {
      if ((new_pos+name_len == NULL) ||
          (*(new_pos+name_len) == '+') ||
          (*(new_pos+name_len) == '/'))
        {
        rc = TRUE;
        break;
        }
      }

    cur_pos = new_pos+1;
    }

  return(rc);
  } /* END node_in_exechostlist() */




int kill_job_on_mom(

  char           *jobid,
  struct pbsnode *pnode)

  {
  struct batch_request *preq;
  int                   rc = -1;
  int                   conn;
  int                   local_errno = 0;
  char                  log_buf[LOCAL_LOG_BUF_SIZE];

  /* job is reported by mom but server has no record of job */
  sprintf(log_buf, "stray job %s found on %s", jobid, pnode->nd_name);
  log_err(-1, __func__, log_buf);
  
  conn = svr_connect(pnode->nd_addrs[0], pnode->nd_mom_port, &local_errno, pnode, NULL, ToServerDIS);

  if (conn >= 0)
    {
    if ((preq = alloc_br(PBS_BATCH_SignalJob)) == NULL)
      {
      log_err(-1, __func__, "unable to allocate SignalJob request-trouble!");
      svr_disconnect(conn);
      }
    else
      {
      strcpy(preq->rq_ind.rq_signal.rq_jid, jobid);
      strcpy(preq->rq_ind.rq_signal.rq_signame, "SIGKILL");
      
      unlock_node(pnode, __func__, NULL, 0);
      rc = issue_Drequest(conn, preq);
      free_br(preq);
      lock_node(pnode, __func__, NULL, 0);
      }
    }

  return(rc);
  } /* END kill_job_on_mom() */




int job_should_be_on_node(
    
  char           *jobid,
  struct pbsnode *pnode)

  {
  int  should_be_on_node = TRUE;
  job *pjob;

  if (strstr(jobid, server_name) != NULL)
    {
    if ((is_job_on_node(pnode, jobid)) == FALSE)
      {
      /* must lock the job before the node */
      unlock_node(pnode, __func__, NULL, 0);
      pjob = svr_find_job(jobid, TRUE);
      lock_node(pnode, __func__, NULL, 0);
      
      if (pjob != NULL)
        {
        if (pjob->ji_qs.ji_state == JOB_STATE_COMPLETE)
          {
          unlock_ji_mutex(pjob, __func__, "A", LOGLEVEL);
          should_be_on_node = FALSE;
          }
        /* job exists, but doesn't currently have resources assigned
         * to this node double check the job struct because we
         * could be in the middle of moving the job around because
         * of data staging, suspend, or rerun */            
        else if (pjob->ji_wattr[JOB_ATR_exec_host].at_val.at_str == NULL)
          {
          unlock_ji_mutex(pjob, __func__, "1", LOGLEVEL);
          
          should_be_on_node = FALSE;
          }
        else if (node_in_exechostlist(pnode->nd_name, pjob->ji_wattr[JOB_ATR_exec_host].at_val.at_str) == FALSE)
          {
          unlock_ji_mutex(pjob, __func__, "2", LOGLEVEL);
          
          should_be_on_node = FALSE;
          }
        else
          unlock_ji_mutex(pjob, __func__, "3", LOGLEVEL);
        }
      else
        should_be_on_node = FALSE;
      }
    }

  return(should_be_on_node);
  } /* END job_should_be_on_node() */




void *finish_job(

  void *vp)

  {
  char *jobid = (char *)vp;
  job  *pjob;

  if (jobid == NULL)
    return(NULL);
  else if ((pjob = svr_find_job(jobid, TRUE)) == NULL)
    {
    free(jobid);

    return(NULL);
    }

  free(jobid);

  free_nodes(pjob);

  svr_setjobstate(pjob, JOB_STATE_COMPLETE, JOB_SUBSTATE_COMPLETE, FALSE);

  handle_complete_first_time(pjob);

  return(NULL);
  } /* END finish_job() */




int remove_jobs_that_have_disappeared(

  struct pbsnode  *pnode,
  resizable_array *reported_ms_jobs,
  time_t           timestamp)

  {
  int   iter = -1;
  char  log_buf[LOCAL_LOG_BUF_SIZE];
  char *jobid;
  
  while ((jobid = (char *)pop_thing(pnode->nd_ms_jobs)) != NULL)
    {
    job *pjob;

    /* locking priority is job before node */
    unlock_node(pnode, __func__, NULL, 0);
    pjob = svr_find_job(jobid, TRUE);
    lock_node(pnode, __func__, NULL, 0);

    if (pjob == NULL)
      {
      free(jobid);

      continue;
      }

    /* 45 seconds is typically the time between intervals for each update 
     * from the mom. Add this in case a stale update is processed and the job
     * hadn't started at the time the update was sent */
    if ((pjob->ji_qs.ji_state >= JOB_STATE_EXITING) ||
        (pjob->ji_qs.ji_substate < JOB_SUBSTATE_RUNNING) ||
        (pjob->ji_wattr[JOB_ATR_start_time].at_val.at_long > timestamp - 45))
      {
      free(jobid);

      unlock_ji_mutex(pjob, __func__, "1", LOGLEVEL);
      continue;
      }

    unlock_ji_mutex(pjob, __func__, "2", LOGLEVEL);
    /* mom didn't report this job - it has exited unexpectedly */
    snprintf(log_buf, sizeof(log_buf),
      "Server thinks job %s is on node %s, but the mom doesn't. Terminating job",
      jobid, pnode->nd_name);
    log_event(PBSEVENT_JOB, PBS_EVENTCLASS_JOB, jobid, log_buf);  

    enqueue_threadpool_request(finish_job, jobid);
    }

  /* now replace the old resizable array with the ones currently reported */
  while ((jobid = (char *)next_thing(reported_ms_jobs, &iter)) != NULL)
    insert_thing(pnode->nd_ms_jobs, jobid);

  return(PBSE_NONE);
  } /* END remove_jobs_that_have_disappeared() */




/*
 * sync_node_jobs() - determine if a MOM has a stale job and possibly delete it
 *
 * This function is called every time we get a node stat from the pbs_mom.
 *
 * NOTE: changed to be processed in a thread so that processing here doesn't hinder
 * the server's ability to reply to the status
 *
 * @see is_stat_get()
 */

void *sync_node_jobs(

  void *vp)

  {
  struct pbsnode       *np;
  sync_job_info        *sji = (sync_job_info *)vp;
  char                 *raw_input;
  char                 *node_id;
  char                 *jobstring_in;
  char                 *joblist;
  char                 *jobidstr;
  /*resizable_array      *ms_jobs;*/

  if (vp == NULL)
    return(NULL);

  raw_input = sji->input;

  /* raw_input's format is:
   *   node name:<JOBID>[ <JOBID>]... */
  if ((jobstring_in = strchr(raw_input, ':')) != NULL)
    {
    node_id = raw_input;
    *jobstring_in = '\0';
    jobstring_in++;
    }
  else
    {
    /* bad input */
    free(raw_input);
    free(sji);

    return(NULL);
    }
    
  if ((np = find_nodebyname(node_id)) == NULL)
    {
    free(raw_input);
    free(sji);

    return(NULL);
    }

  /* FORMAT <JOBID>[ <JOBID>]... */
  joblist = jobstring_in;
  jobidstr = threadsafe_tokenizer(&joblist, " ");
  
  /*ms_jobs = initialize_resizable_array(MAX(np->nd_ms_jobs->num, 2));*/

  while ((jobidstr != NULL) && 
         (isdigit(*jobidstr)) != FALSE)
    {
    if (job_should_be_on_node(jobidstr, np) == FALSE)
      {
      kill_job_on_mom(jobidstr, np);
      }
    else
      {
/*      char *jobid = remove_thing_memcmp(np->nd_ms_jobs, jobidstr, strlen(jobidstr));

      if (jobid != NULL)
        insert_thing(ms_jobs, jobid);*/
      }
    
    jobidstr = threadsafe_tokenizer(&joblist, " ");
    } /* END while ((jobidstr != NULL) && ...) */

  /* SUCCESS */
  free(raw_input);

  /* now check if the mom is reporting any jobs that the server doesn't know about */
  /*remove_jobs_that_have_disappeared(np, ms_jobs, sji->timestamp);*/

  free(sji);

  /*free_resizable_array(ms_jobs);*/

  unlock_node(np, __func__, NULL, 0);

  return(NULL);
  }  /* END sync_node_jobs() */




/*
 * update_job_data() - update job with values passed through "jobdata"
 *
 * This function is called every time we get a "jobdata" status from the pbs_mom.
 *
 * @see is_stat_get()
 */

void update_job_data(

  struct pbsnode *np,            /* I */
  char           *jobstring_in)  /* I (changed attributes sent by mom) */

  {
  char       *jobdata;
  char       *jobdata_ptr;
  char       *jobidstr;
  char       *attr_name;
  char       *attr_value;
  char        log_buf[LOCAL_LOG_BUF_SIZE];

  struct job *pjob = NULL;
  int         on_node = FALSE;

  if ((jobstring_in == NULL) || (!isdigit(*jobstring_in)))
    {
    /* NO-OP */

    return;
    }

  /* FORMAT <JOBID>:<atrtributename=value>,<atrtributename=value>... */

  jobdata = strdup(jobstring_in);
  jobdata_ptr = jobdata;

  jobidstr = threadsafe_tokenizer(&jobdata_ptr, ":");

  if ((jobidstr != NULL) && isdigit(*jobidstr))
    {
    if (strstr(jobidstr, server_name) != NULL)
      {
      on_node = is_job_on_node(np, jobidstr);
      pjob = svr_find_job(jobidstr, TRUE);

      if (pjob != NULL)
        {
        int bad;

        svrattrl tA;
        
        /* job exists, so get the attributes and update them */
        attr_name = threadsafe_tokenizer(&jobdata_ptr, "=");
        
        while (attr_name != NULL)
          {
          attr_value = threadsafe_tokenizer(&jobdata_ptr, ",");
          
          if (LOGLEVEL >= 9)
            {
            sprintf(log_buf, "Mom sent changed attribute %s value %s for job %s",
              attr_name,
              attr_value,
              pjob->ji_qs.ji_jobid);
              
            log_event(PBSEVENT_JOB,PBS_EVENTCLASS_JOB,pjob->ji_qs.ji_jobid,log_buf);  
            }
          
          memset(&tA, 0, sizeof(tA));

          tA.al_name  = attr_name;
          tA.al_resc  = "";
          tA.al_value = attr_value;
          tA.al_op    = SET;

          modify_job_attr(
            pjob,
            &tA,                              /* I: ATTR_sched_hint - svrattrl */
            ATR_DFLAG_MGWR | ATR_DFLAG_SvWR,
            &bad);

          attr_name = threadsafe_tokenizer(&jobdata_ptr, "=");
          }
        
        unlock_ji_mutex(pjob, __func__, "1", LOGLEVEL);
        }
      
      if (on_node == FALSE)
        {
        /* job is reported by mom but server has no record of job */
        sprintf(log_buf, "stray job %s reported on %s", jobidstr, np->nd_name);

        log_err(-1, __func__, log_buf);
        }
      }
    }

  free(jobdata);
  }  /* END update_job_data() */




/*
 *      setup_notification -  Sets up the  mechanism for notifying
 *                            other members of the server's node
 *                            pool that a new node was added manually
 *                            via qmgr.  Actual notification occurs some
 *                            time later through the send_cluster_addrs mechanism
 */

void setup_notification(
    
  char *pname) /* I */

  {
  struct pbsnode *pnode;
  new_node       *nnew;

  if (pname != NULL)
    {
    if ((pnode = find_nodebyname(pname)) != NULL)
      {
      /* call it offline until after all nodes get the new ipaddr */
      pnode->nd_state |= INUSE_OFFLINE;
      
      nnew = calloc(1, sizeof(new_node));
      
      if (nnew == NULL)
        {
        unlock_node(pnode, "setup_notification", "nnew == NULL", LOGLEVEL);
        return;
        }
      
      CLEAR_LINK(nnew->nn_link);
      
      nnew->nn_name = strdup(pname);
      
      append_link(&svr_newnodes, &nnew->nn_link, nnew);
      
      unlock_node(pnode, "setup_notification", "nnew != NULL", LOGLEVEL);
      }
    }

  if (addrnote_mutex == NULL)
    {
    addrnote_mutex = calloc(1, sizeof(pthread_mutex_t));
    pthread_mutex_init(addrnote_mutex,NULL);
    }

  pthread_mutex_lock(addrnote_mutex);
  num_addrnote_tasks++;
  pthread_mutex_unlock(addrnote_mutex);

  return;
  }


/* 
 * reads all of the status information from stream
 * and stores it in a dynamic string
 */

dynamic_string *get_status_info(

    struct tcp_chan *chan)

  {
  dynamic_string *ds = get_dynamic_string(-1, NULL);
  char           *ret_info;
  int             rc;

  if (ds == NULL)
    return(NULL);
  
  while (((ret_info = disrst(chan, &rc)) != NULL) && 
         (rc == DIS_SUCCESS))
    {
    if (!strcmp(ret_info, IS_EOL_MESSAGE))
      {
      free(ret_info);
      break;
      }

    copy_to_end_of_dynamic_string(ds, ret_info);
    free(ret_info);
    }

  /* clear the transmission */
/*  DIS_tcp_reset(chan,0); */

  return(ds);
  } /* END get_status_info() */



/*
 * switches the current node to the desired
 * numa subnode requested in str and unlocks np
 */

struct pbsnode *get_numa_from_str(
    
  char           *str, /* I */
  struct pbsnode *np)  /* I */

  {
  char           *numa_id;
  struct pbsnode *numa;
  unsigned long   numa_index;
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  
  if (np->node_boards == NULL)
    {
    /* ERROR */
    snprintf(log_buf,sizeof(log_buf),
      "Node %s isn't declared to be NUMA, but mom is reporting\n",
      np->nd_name);
    log_err(-1, __func__, log_buf);
  
    unlock_node(np, __func__, "np numa update", LOGLEVEL);
    
    return(NULL);
    }
  
  numa_id = str + strlen(NUMA_KEYWORD);
  numa_index = atoi(numa_id);
  
  numa = AVL_find(numa_index, np->nd_mom_port, np->node_boards);
  
  if (numa == NULL)
    {
    /* ERROR */
    snprintf(log_buf,sizeof(log_buf),
      "Could not find NUMA index %lu for node %s\n",
      numa_index,
      np->nd_name);
    log_err(-1, __func__, log_buf);
    
    unlock_node(np, __func__, "np numa update", LOGLEVEL);
    
    return(NULL);
    }
 
  /* SUCCESS */
  unlock_node(np, __func__, "np numa update", LOGLEVEL);
  lock_node(numa, __func__, "numa numa update", LOGLEVEL);
  
  numa->nd_lastupdate = time(NULL);
  
  return(numa);
  } /* END get_numa_from_str() */



/* 
 * unlocks np and returns a the node specified in string, locked
 */

struct pbsnode *get_node_from_str(

  char           *str,     /* I */
  char           *orig_id, /* I */
  struct pbsnode *np)      /* M */

  {
  /* this is a node reporting on another node as well */
  char           *node_id = str + strlen("node=");
  struct pbsnode *next = NULL;
  char            log_buf[LOCAL_LOG_BUF_SIZE];
 
  /* don't do anything if the name is the same as this node's name */
  if (strcmp(node_id, np->nd_name))
    {
    unlock_node(np, __func__, "np not numa update", LOGLEVEL);
    
    next = find_nodebyname(node_id);
    
    if (next == NULL)
      {
      /* NYI: should we add logic here to attempt the canonical name if this 
       * is the short name, and attempt the short name if this is the 
       * canonical name? */
      
      /* ERROR */
      snprintf(log_buf,sizeof(log_buf),
        "Node %s is reporting on node %s, which pbs_server doesn't know about\n",
        orig_id,
        node_id);
      log_err(-1, __func__, log_buf);
      }
    else
      {
      if (LOGLEVEL >= 7)
        {
        snprintf(log_buf,sizeof(log_buf),
          "Node %s is reporting for node %s\n",
          orig_id,
          node_id);
        
        log_event(PBSEVENT_SYSTEM, PBS_EVENTCLASS_SERVER, __func__, log_buf);
        }
      
      next->nd_lastupdate = time(NULL);
      }
    }
  else
    {
    next = np;
    next->nd_lastupdate = time(NULL);
    }

  /* next may be NULL */

  return(next);
  } /* END get_node_from_str() */




int handle_auto_np(

  struct pbsnode *np,  /* M */
  char           *str) /* I */

  {
  pbs_attribute nattr;
  
  /* first we decode str into nattr... + 6 is because str has format
   * ncpus=X, and 6 = strlen(ncpus=) */  
  if ((node_attr_def + ND_ATR_np)->at_decode(&nattr, ATTR_NODE_np, NULL, str + 6, 0) == 0)
    {
    /* ... and if MOM's ncpus is different than our np... */
    if (nattr.at_val.at_long != np->nd_nsn)
      {
      /* ... then we do the defined magic to create new subnodes */
      (node_attr_def + ND_ATR_np)->at_action(&nattr, (void *)np, ATR_ACTION_ALTER);
      
      update_nodes_file(np);
      }
    }

  return(PBSE_NONE);
  } /* END handle_auto_np() */




int process_uname_str(

  struct pbsnode *np,
  char           *str)

  {
  /* for any mom mode if an address did not succeed at getnameinfo it was
   * given the hex value of its ip address */
  if (!strncmp(np->nd_name, "0x", 2))
    {
    char *cp;
    char  node_name[PBS_MAXHOSTNAME + 1];
    int   count;
    
    cp = strchr(str, ' ');
    count = 0;
    
    do
      {
      cp++;
      node_name[count] = *cp;
      count++;
      } while (*cp != ' ' && count < PBS_MAXHOSTNAME);
    
    node_name[count-1] = 0;
    cp = strdup(node_name);
    free(np->nd_name);
    np->nd_name = cp;
    np->nd_first = init_prop(np->nd_name);
    np->nd_last = np->nd_first;
    np->nd_f_st = init_prop(np->nd_name);
    np->nd_l_st = np->nd_f_st;
    }

  return(PBSE_NONE);
  } /* END process_uname_str() */




int process_state_str(

  struct pbsnode *np,
  char           *str)

  {
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  struct pbssubn *sp = NULL;
  int             rc = PBSE_NONE;

  if (!strncmp(str, "state=down", 10))
    {
    update_node_state(np, INUSE_DOWN);
    }
  else if (!strncmp(str, "state=busy", 10))
    {
    update_node_state(np, INUSE_BUSY);
    }
  else if (!strncmp(str, "state=free", 10))
    {
    update_node_state(np, INUSE_FREE);
    }
  else
    {
    sprintf(log_buf, "unknown %s from node %s",
      str,
      (np->nd_name != NULL) ? np->nd_name : "NULL");
    
    log_err(-1, __func__, log_buf);
    
    update_node_state(np, INUSE_UNKNOWN);
    }
  
  if (LOGLEVEL >= 9)
    {
    sprintf(log_buf, "node '%s' is at state '0x%x'\n",
      np->nd_name,
      np->nd_state);
    
    log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, log_buf);
    }
  
  for (sp = np->nd_psn; sp != NULL; sp = sp->next)
    {
    if ((!(np->nd_state & INUSE_OFFLINE)) &&
        (sp->inuse & INUSE_OFFLINE))
      {
      /* this doesn't seem to ever happen */
      if (LOGLEVEL >= 2)
        {
        sprintf(log_buf, "sync'ing subnode state '%s' with node state on node %s\n",
          "offline",
          np->nd_name);
        
        log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, log_buf);
        }
      
      sp->inuse &= ~INUSE_OFFLINE;
      }
    
    sp->inuse &= ~INUSE_DOWN;
    }

  return(rc);
  } /* END process_state_str() */



int save_node_status(

  struct pbsnode *np,
  pbs_attribute  *temp)

  {
  int  rc = PBSE_NONE;
  char date_attrib[MAXLINE];

  /* it's nice to know when the last update happened */
  snprintf(date_attrib, sizeof(date_attrib), "rectime=%ld", (long)time(NULL));
  
  if (decode_arst(temp, NULL, NULL, date_attrib, 0))
    {
    DBPRT(("is_stat_get:  cannot add date_attrib\n"));
    }
  
  /* insert the information from "temp" into np */
  if ((rc = node_status_list(temp, np, ATR_ACTION_ALTER)) != PBSE_NONE)
    {
    DBPRT(("is_stat_get: cannot set node status list\n"));
    }

  free_arst(temp);

  return(rc);
  } /* END save_node_status() */



#ifdef NVIDIA_GPUS
/*
 **     reset gpu data in case mom reconnects with changed gpus.
 **     If we have real gpus, not virtual ones, then clear out gpu_status,
 **     gpus count and remove gpu subnodes.
 */

void clear_nvidia_gpus(

  struct pbsnode *np)  /* I */

  {
  pbs_attribute   temp;

  if ((np->nd_gpus_real) && (np->nd_ngpus > 0))
    {
    /* delete gpusubnodes by freeing it */
    free(np->nd_gpusn);
    np->nd_gpusn = NULL;

    /* reset # of gpus, etc */
    np->nd_ngpus = 0;
    np->nd_ngpus_free = 0;

    /* unset "gpu_status" node attribute */

    memset(&temp, 0, sizeof(temp));

    if (decode_arst(&temp, NULL, NULL, NULL, 0))
      {
      log_err(-1, __func__, "clear_nvidia_gpus:  cannot initialize attribute\n");

      return;
      }

    node_gpustatus_list(&temp, np, ATR_ACTION_ALTER);
    }

  return;
  }  /* END clear_nvidia_gpus() */

#endif  /* NVIDIA_GPUS */




int process_status_info(

  char           *nd_name,
  dynamic_string *status_info)

  {
  char           *str;
  char           *name = nd_name;
  struct pbsnode *current;
  long            mom_job_sync = FALSE;
  long            auto_np = FALSE;
  long            down_on_error = FALSE;
  int             dont_change_state = FALSE;
  pbs_attribute   temp;
  int             rc = PBSE_NONE;
  int             send_hello = FALSE;

  get_svr_attr_l(SRV_ATR_MomJobSync, &mom_job_sync);
  get_svr_attr_l(SRV_ATR_AutoNodeNP, &auto_np);
  get_svr_attr_l(SRV_ATR_DownOnError, &down_on_error);

  /* Before filling the "temp" pbs_attribute, initialize it.
   * The second and third parameter to decode_arst are never
   * used, so just leave them empty. (GBS) */
  memset(&temp, 0, sizeof(temp));

  if ((rc = decode_arst(&temp, NULL, NULL, NULL, 0)) != PBSE_NONE)
    {
    log_record(PBSEVENT_DEBUG, PBS_EVENTCLASS_NODE, __func__, "cannot initialize attribute");
    return(rc);
    }

  /* if original node cannot be found do not process the update */
  if ((current = find_nodebyname(nd_name)) == NULL)
    return(PBSE_NONE);

  /* loop over each string */
  for (str = status_info->str; str != NULL && *str; str += strlen(str) + 1)
    {
    /* these two options are for switching nodes */
    if (!strncmp(str, NUMA_KEYWORD, strlen(NUMA_KEYWORD)))
      {
      /* if we've already processed some, save this before moving on */
      if (str != status_info->str)
        save_node_status(current, &temp);
      
      dont_change_state = FALSE;

      if ((current = get_numa_from_str(str, current)) == NULL)
        break;
      else
        continue;
      }
    else if (!strncmp(str, "node=", strlen("node=")))
      {
      /* if we've already processed some, save this before moving on */
      if (str != status_info->str)
        save_node_status(current, &temp);

      dont_change_state = FALSE;

      if ((current = get_node_from_str(str, name, current)) == NULL)
        break;
      else
        continue;
      }

    /* add the info to the "temp" pbs_attribute */
#ifdef NVIDIA_GPUS
    else if (!strcmp(str, START_GPU_STATUS))
      {
      is_gpustat_get(current, &str);
      }
#endif
    else if (!strcmp(str, "first_update=true"))
      {
      /* mom is requesting that we send the mom hierarchy file to her */
      remove_hello(&hellos, current->nd_name);
      send_hello = TRUE;
#ifdef NVIDIA_GPUS
      /* reset gpu data in case mom reconnects with changed gpus */
      clear_nvidia_gpus(current);
#endif  /* NVIDIA_GPUS */
      }
    else if ((rc = decode_arst(&temp, NULL, NULL, str, 0)) != PBSE_NONE)
      {
      DBPRT(("is_stat_get: cannot add attributes\n"));

      free_arst(&temp);

      break;
      }

    if (!strncmp(str, "state", 5))
      {
      if (dont_change_state == FALSE)
        process_state_str(current, str);
      }
    else if ((allow_any_mom == TRUE) &&
             (!strncmp(str, "uname", 5))) 
      {
      process_uname_str(current, str);
      }
    else if (!strncmp(str, "me", 2))  /* shorter str compare than "message" */
      {
      if ((!strncmp(str, "message=ERROR", 13)) &&
          (down_on_error == TRUE))
        {
        update_node_state(current, INUSE_DOWN);
        dont_change_state = TRUE;
        }
      }
    else if ((mom_job_sync == TRUE) &&
             (!strncmp(str, "jobdata=", 8)))
      {
      /* update job attributes based on what the MOM gives us */      
      update_job_data(current, str + strlen("jobdata="));
      }
    else if ((mom_job_sync == TRUE) &&
             (!strncmp(str, "jobs=", 5)))
      {
      /* walk job list reported by mom */
      size_t         len = strlen(str) + strlen(current->nd_name) + 2;
      char          *jobstr = calloc(1, len);
      sync_job_info *sji = calloc(1, sizeof(sync_job_info));

      if (jobstr != NULL)
        {
        sprintf(jobstr, "%s:%s", current->nd_name, str+5);
        sji->input = jobstr;
        sji->timestamp = time(NULL);
        /* jobstr must be freed in sync_node_jobs */
        enqueue_threadpool_request(sync_node_jobs, sji);
        }
      }
    else if (auto_np)
      {
      if (!(strncmp(str, "ncpus=", 6)))
        {
        handle_auto_np(current, str);
        }
      }
    } /* END processing strings */

  if (current != NULL)
    {
    save_node_status(current, &temp);
    unlock_node(current, __func__, NULL, 0);
    }
  
  if ((rc == PBSE_NONE) &&
      (send_hello == TRUE))
    rc = SEND_HELLO;
    
  return(rc);
  } /* END process_status_info() */




int is_reporter_node(

  char *node_id)

  {
  struct pbsnode *pnode = find_nodebyname(node_id);
  int             rc = FALSE;

  if (pnode != NULL)
    {
    rc = pnode->nd_is_alps_reporter;
    unlock_node(pnode, __func__, NULL, 0);
    }

  return(rc);
  } /* END is_reporter_node() */




int is_stat_get(

  char            *node_name,
  struct tcp_chan *chan)

  {
  int             rc;
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  dynamic_string *status_info;

  if (LOGLEVEL >= 3)
    {
    sprintf(log_buf, "received status from node %s", node_name);
    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  status_info = get_status_info(chan);
 
  if (is_reporter_node(node_name))
    rc = process_alps_status(node_name, status_info);
  else
    rc = process_status_info(node_name, status_info);

  free_dynamic_string(status_info);

  return(rc);
  }  /* END is_stat_get() */



#ifdef NVIDIA_GPUS
/*
 * Function to check if there is a job assigned to this gpu
 */

int gpu_has_job(

  struct pbsnode *pnode,
  int  gpuid)

  {
  job            *pjob;
  char           *gpu_str;
  char           *found_str;
  /* increased so that really high gpu indexes don't bother us */
  char            tmp_str[PBS_MAXHOSTNAME + 10];
  struct pbssubn *np;
  struct jobinfo *jp;

  /* check each subnode for a job using a gpuid */
  for (np = pnode->nd_psn;np != NULL;np = np->next)
    {
    /* for each jobinfo on subnode on node ... */
    for (jp = np->jobs;jp != NULL;jp = jp->next)
      {
      if (jp->jobid != NULL)
        {
        if ((pjob = get_job_from_jobinfo(jp,pnode)) != NULL)
          {
          /* Does this job have this gpuid assigned? */
          if ((pjob->ji_qs.ji_state == JOB_STATE_RUNNING) &&
              (pjob->ji_wattr[JOB_ATR_exec_gpus].at_flags & ATR_VFLAG_SET) != 0)
            {
            gpu_str = pjob->ji_wattr[JOB_ATR_exec_gpus].at_val.at_str;
            
            if (gpu_str != NULL)
              {
              snprintf(tmp_str, sizeof(tmp_str), "%s-gpu/%d",
                pnode->nd_name, gpuid);
              
              /* look thru the string and see if it has this host and gpuid.
               * exec_gpus string should be in format of 
               * <hostname>-gpu/<index>[+<hostname>-gpu/<index>...]
               */
              
              found_str = strstr (gpu_str, tmp_str);
              if (found_str != NULL)
                {
                unlock_ji_mutex(pjob, __func__, "1", LOGLEVEL);
                return(TRUE);
                }
              }
            }
          
          /* done with job, unlock the mutex */
          unlock_ji_mutex(pjob, __func__, "2", LOGLEVEL);
          }
        }
      } /* END for each job on the subnode */
    } /* END for each subnode */

  return(FALSE);
  }
#endif  /* NVIDIA_GPUS */




#ifdef NVIDIA_GPUS
char *move_past_gpu_status(

  char *str)

  {
  while ((str != NULL) &&
         (str[0] != '\0'))
    {
    if (!strcmp(str, END_GPU_STATUS))
      break;

    str += strlen(str) + 1;
    }

  return(str);
  } /* END move_past_gpu_status() */
#endif




#ifdef NVIDIA_GPUS
/*
 * Function to process gpu status messages received from the mom
 */

int is_gpustat_get(

  struct pbsnode  *np,      /* I (modified) */
  char           **str_ptr) /* I (modified) */

  {
  int            rc;
  pbs_attribute  temp;
  char          *gpuid = NULL;
  char          *str = *str_ptr;
  char           log_buf[LOCAL_LOG_BUF_SIZE];
  int            gpuidx = -1;
  char           gpuinfo[2048];
  int            need_delimiter = FALSE;
  int            reportedgpucnt = 0;
  int            startgpucnt = 0;
  int            drv_ver = 0;

  if (LOGLEVEL >= 7)
    {
    sprintf(log_buf, "received gpu status from node %s",
      (np != NULL) ? np->nd_name : "NULL");

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  /* save current gpu count for node */
  startgpucnt = np->nd_ngpus;

  /*
   *  Before filling the "temp" pbs_attribute, initialize it.
   *  The second and third parameter to decode_arst are never
   *  used, so just leave them empty. (GBS)
   */

  memset(&temp, 0, sizeof(temp));
  memset(gpuinfo, 0, 2048);

  rc = DIS_SUCCESS;

  if (decode_arst(&temp, NULL, NULL, NULL, 0))
    {
    DBPRT(("is_gpustat_get:  cannot initialize attribute\n"));

    return(DIS_NOCOMMIT);
    }

  str += strlen(str) + 1;

  for (; str != NULL && *str; str += strlen(str) + 1)
    {
    /* add the info to the "temp" attribute */

    /* get timestamp */
    if (!strncmp(str, "timestamp=", 10))
      {
      if (decode_arst(&temp, NULL, NULL, str, 0))
        {
        DBPRT(("is_gpustat_get: cannot add attributes\n"));

        free_arst(&temp);
        *str_ptr = move_past_gpu_status(str);

        return(DIS_NOCOMMIT);
        }
      continue;
      }

    /* get driver version, if there is one */
    if (!strncmp(str, "driver_ver=", 11))
      {
      if (decode_arst(&temp, NULL, NULL, str, 0))
        {
        DBPRT(("is_gpustat_get: cannot add attributes\n"));

        free_arst(&temp);
        *str_ptr = move_past_gpu_status(str);

        return(DIS_NOCOMMIT);
        }
      drv_ver = atoi(str + 11);
      continue;
      }
    else if (!strcmp(str, END_GPU_STATUS))
      {
      break;
      }

    /* gpuid must come before the rest or we will be in trouble */

    if (!strncmp(str, "gpuid=", 6))
      {
      if (strlen(gpuinfo) > 0)
        {
        if (decode_arst(&temp, NULL, NULL, gpuinfo, 0))
          {
          DBPRT(("is_gpustat_get: cannot add attributes\n"));

          free_arst(&temp);
          *str_ptr = move_past_gpu_status(str);

          return(DIS_NOCOMMIT);
          }
        memset(gpuinfo, 0, 2048);
        }

      gpuid = &str[6];

      /*
       * Get this gpus index, if it does not yet exist then find an empty entry.
       * We need to allow for the gpu status results being returned in
       * different orders since the nvidia order may change upon mom's reboot
       */

      gpuidx = gpu_entry_by_id(np, gpuid, TRUE);
      if (gpuidx == -1)
        {
        /*
         * Failure - we could not get / create a nd_gpusn entry for this gpu,
         * log an error message.
         */

        if (LOGLEVEL >= 3)
          {
          sprintf(log_buf,
            "Failed to get/create entry for gpu %s on node %s\n",
            gpuid,
            np->nd_name);

          log_ext(-1, __func__, log_buf, LOG_DEBUG);
          }

        free_arst(&temp);
        *str_ptr = move_past_gpu_status(str);

        return(DIS_SUCCESS);
        }

      sprintf(gpuinfo, "gpu[%d]=gpu_id=%s;", gpuidx, gpuid);
      need_delimiter = FALSE;
      reportedgpucnt++;
      np->nd_gpusn[gpuidx].driver_ver = drv_ver;

      /* mark that this gpu node is not virtual */
      np->nd_gpus_real = TRUE;
      
      /*
       * if we have not filled in the gpu_id returned by the mom node
       * then fill it in
       */
      if ((gpuidx >= 0) && (np->nd_gpusn[gpuidx].gpuid == NULL))
        {
        np->nd_gpusn[gpuidx].gpuid = strdup(gpuid);
        }      

      }
    else
      {
      if (need_delimiter)
        {
        strcat(gpuinfo, ";");
        }
      strcat(gpuinfo, str);
      need_delimiter = TRUE;
      }

    /* check current gpu mode and determine gpu state */
    
    if (!memcmp(str, "gpu_mode=", 9))
      {
      if ((!memcmp(str + 9, "Normal", 6)) || (!memcmp(str + 9, "Default", 7)))
        {
        np->nd_gpusn[gpuidx].mode = gpu_normal;
        if (gpu_has_job(np, gpuidx))
          {
          np->nd_gpusn[gpuidx].state = gpu_shared;
          }
        else
          {
          np->nd_gpusn[gpuidx].inuse = 0;
          np->nd_gpusn[gpuidx].state = gpu_unallocated;
          }
        }
      else if ((!memcmp(str + 9, "Exclusive", 9)) ||
              (!memcmp(str + 9, "Exclusive_Thread", 16)))
        {
        np->nd_gpusn[gpuidx].mode = gpu_exclusive_thread;
        if (gpu_has_job(np, gpuidx))
          {
          np->nd_gpusn[gpuidx].state = gpu_exclusive;
          }
        else
          {
          np->nd_gpusn[gpuidx].inuse = 0;
          np->nd_gpusn[gpuidx].state = gpu_unallocated;
          }
        }
      else if (!memcmp(str + 9, "Exclusive_Process", 17))
        {
        np->nd_gpusn[gpuidx].mode = gpu_exclusive_process;
        if (gpu_has_job(np, gpuidx))
          {
          np->nd_gpusn[gpuidx].state = gpu_exclusive;
          }
        else
          {
          np->nd_gpusn[gpuidx].inuse = 0;
          np->nd_gpusn[gpuidx].state = gpu_unallocated;
          }
        }
      else if (!memcmp(str + 9, "Prohibited", 10))
        {
        np->nd_gpusn[gpuidx].mode = gpu_prohibited;
        np->nd_gpusn[gpuidx].state = gpu_unavailable;
        }
      else
        {
        /* unknown mode, default to prohibited */
        np->nd_gpusn[gpuidx].mode = gpu_prohibited;
        np->nd_gpusn[gpuidx].state = gpu_unavailable;
        if (LOGLEVEL >= 3)
          {
          sprintf(log_buf,
            "GPU %s has unknown mode on node %s",
            gpuid,
            np->nd_name);

          log_ext(-1, __func__, log_buf, LOG_DEBUG);
          }
        }
 
      /* add gpu_mode so it gets added to the pbs_attribute */

      if (need_delimiter)
        {
        strcat(gpuinfo, ";");
        }

      switch (np->nd_gpusn[gpuidx].state)
        {
        case gpu_unallocated:
          strcat (gpuinfo, "gpu_state=Unallocated");
          break;
        case gpu_shared:
          strcat (gpuinfo, "gpu_state=Shared");
          break;
        case gpu_exclusive:
          strcat (gpuinfo, "gpu_state=Exclusive");
          break;
        case gpu_unavailable:
          strcat (gpuinfo, "gpu_state=Unavailable");
          break;
        }
      }

    } /* end of while disrst */

  if (strlen(gpuinfo) > 0)
    {
    if (decode_arst(&temp, NULL, NULL, gpuinfo, 0))
      {
      DBPRT(("is_gpustat_get: cannot add attributes\n"));
      
      free_arst(&temp);
      *str_ptr = move_past_gpu_status(str);

      return(DIS_NOCOMMIT);
      }
    }

  /* maintain the gpu count, if it has changed we need to update the nodes file */

  if (reportedgpucnt != startgpucnt)
    {
    np->nd_ngpus = reportedgpucnt;

    /* update the nodes file */
    update_nodes_file(np);
    }

  node_gpustatus_list(&temp, np, ATR_ACTION_ALTER);
  *str_ptr = move_past_gpu_status(str);

  return(DIS_SUCCESS);
  }  /* END is_gpustat_get() */
#endif  /* NVIDIA_GPUS */



/*
** Start a standard inter-server message.
*/

int is_compose(

  struct tcp_chan *chan,  /* I */
  int command) /* I */

  {
  int ret;

  ret = diswsi(chan, IS_PROTOCOL);

  if (ret != DIS_SUCCESS)
    goto done;

  ret = diswsi(chan, IS_PROTOCOL_VER);

  if (ret != DIS_SUCCESS)
    goto done;

  ret = diswsi(chan, command);

  if (ret != DIS_SUCCESS)
    goto done;

  return(DIS_SUCCESS);

done:

  DBPRT(("is_compose: send error %s\n",
         dis_emsg[ret]))

  return(ret);
  }  /* END is_compose() */




/* EOF on a stream received (either stream or addr must be specified) */
/* mark node down */
/* NOTE: pass in stream = -1 if you wish the stream to be optional */

void stream_eof(

  int       stream,  /* I (optional) */
  u_long    addr,  /* I (optional) */
  uint16_t  port,  /* I (optional) */
  int       ret)     /* I (ignored) */

  {
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  enum conn_type  cntype = ToServerDIS;
  int             conn;
  int             my_err = 0;

  struct pbsnode *np = NULL;

  if (LOGLEVEL >= 6)
    {
    sprintf(log_buf, "stream: %d, addr: %ld, port %d", stream, addr, port);
    LOG_EVENT(PBSEVENT_JOB, PBS_EVENTCLASS_JOB, __func__, log_buf);
    }

  if (addr != 0)
    {
    np = AVL_find(addr, port, ipaddrs);
    }

  if (np == NULL)
    {
    /* cannot locate node */

    return;
    }

  /* Before we mark this node down see if we can connect */
  lock_node(np, __func__, "parent", LOGLEVEL);
  conn = svr_connect(addr, port, &my_err, np, NULL, cntype);
  if(conn >= 0)
    {
    unlock_node(np, __func__, "parent", LOGLEVEL);
    svr_disconnect(conn);
    return;
    }


  sprintf(log_buf,
    "connection to %s is no longer valid, connection may have been closed remotely, remote service may be down, or message may be corrupt (%s).  setting node state to down",
    np->nd_name,
    dis_emsg[ret]);

  log_err(-1, __func__, log_buf);

  /* mark node and all subnodes as down */

  if (np->num_node_boards > 0)
    {
    int             i;
    struct pbsnode *pnode;

    for (i = 0; i < np->num_node_boards; i++)
      {
      pnode = AVL_find(i,np->nd_mom_port,np->node_boards);

      lock_node(pnode, __func__, "subs", LOGLEVEL);
      update_node_state(pnode,INUSE_DOWN);
      unlock_node(pnode, __func__, "subs", LOGLEVEL);
      }
    }
  else
    update_node_state(np, INUSE_DOWN);

  unlock_node(np, __func__, "parent", LOGLEVEL);

  return;
  }  /* END stream_eof() */




/*
 * wrapper task that check_nodes places in the thread pool's queue
 */
void *check_nodes_work(

  void *vp)

  {
  work_task        *ptask = (struct work_task *)vp;

  struct pbsnode   *np = NULL;
  long              chk_len = 300;
  char              log_buf[LOCAL_LOG_BUF_SIZE];
  time_t            time_now = time(NULL);

  node_iterator     iter;
  
  /* load min refresh interval */
  get_svr_attr_l(SRV_ATR_check_rate, &chk_len);

  if (LOGLEVEL >= 5)
    {
    sprintf(log_buf, "verifying nodes are active (min_refresh = %d seconds)", (int)chk_len);

    log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, log_buf);
    }

  /* evaluate all nodes */
  reinitialize_node_iterator(&iter);

  while ((np = next_node(&allnodes,np,&iter)) != NULL)
    {
    if (!(np->nd_state & INUSE_DOWN))
      {
      if (np->nd_lastupdate < (time_now - chk_len)) 
        {
        if (LOGLEVEL >= 0)
          {
          sprintf(log_buf, "node %s not detected in %ld seconds, marking node down",
            np->nd_name,
            (long int)(time_now - np->nd_lastupdate));
          
          log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, log_buf);
          }
        
        update_node_state(np, (INUSE_DOWN));    
        }
      }
    } /* END for each node */

  if (ptask->wt_parm1 == NULL)
    {
    set_task(WORK_Timed,time_now + chk_len,check_nodes,NULL,FALSE);
    }

  /* since this is done via threading, we now free the task here */
  free(ptask->wt_mutex);
  free(ptask);

  return(NULL);
  } /* check_nodes_work() */




/*
 * Mark any nodes that haven't checked in as down.
 * If the node isn't down then it checks to see that the
 * last update hasn't been too long ago.
 */

void check_nodes(

  struct work_task *ptask)  /* I (modified) */

  {
  int rc = enqueue_threadpool_request(check_nodes_work,ptask);

  if (rc)
    {
    log_err(rc, __func__, "Unable to enqueue check nodes task into the threadpool");
    }
  }  /* END check_nodes() */



/* sync w/#define IS_XXX */

const char *PBSServerCmds2[] =
  {
  "NULL",
  "HELLO",
  "CLUSTER_ADDRS",
  "UPDATE",
  "STATUS",
  "GPU_STATUS",
  NULL
  };



/*************************************************
 * svr_is_request
 *
 * Return: svr_is_request always returns a non-zero value
 *         and it must call close_conn to close the connection
 *         before returning. PBSE_SOCKET_CLOSE is the code
 *         for a successful return. But which ever retun 
 *         code is iused it must terminate the while loop
 *         in start_process_pbs_server_port.
 *************************************************/
int svr_is_request(
    
  struct tcp_chan *chan,
  int              version)

  {
  int                 command = 0;
  int                 ret = DIS_SUCCESS;
  int                 i;
  int                 err;
  char                nodename[PBS_MAXHOSTNAME];
  int                 perm = ATR_DFLAG_MGRD | ATR_DFLAG_MGWR;

  unsigned long       ipaddr;
  unsigned short      mom_port;
  unsigned short      rm_port;
  unsigned long       tmpaddr;

  struct sockaddr_in *addr = NULL;
  struct sockaddr     s_addr;
  unsigned int        len = sizeof(s_addr);

  struct pbsnode     *node = NULL;
  char               *node_name = NULL;

  char                log_buf[LOCAL_LOG_BUF_SIZE+1];

  command = disrsi(chan, &ret);

  if (ret != DIS_SUCCESS)
    goto err;

  if (LOGLEVEL >= 4)
    {
    snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
        "message received from sock %d (version %d)",
        chan->sock,
        version);

    log_event(PBSEVENT_ADMIN,PBS_EVENTCLASS_SERVER,__func__,log_buf);
    }

  if (getpeername(chan->sock, &s_addr, &len) != 0)
    {
    close_conn(chan->sock, FALSE);
    log_err(errno,__func__,"Cannot get socket name using getpeername\n");
    return(PBSE_SOCKET_CLOSE);
    }

  addr = (struct sockaddr_in *)&s_addr;

  if (version != IS_PROTOCOL_VER)
    {
    snprintf(log_buf, LOCAL_LOG_BUF_SIZE, "protocol version %d unknown from %s",
      version,
      netaddr(addr));

    log_err(-1, __func__, log_buf);
    close_conn(chan->sock, FALSE);
    return PBSE_SOCKET_DATA;
    }

  /* check that machine is known */
  mom_port = disrsi(chan, &ret);
  rm_port = disrsi(chan, &ret);

  if (LOGLEVEL >= 3)
    {
    snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
      "message received from addr %s: mom_port %d  - rm_port %d",
      netaddr(addr),
      mom_port,
      rm_port);

    log_event(PBSEVENT_ADMIN,PBS_EVENTCLASS_SERVER,__func__,log_buf);
    }

  ipaddr = ntohl(addr->sin_addr.s_addr);
  
  if ((node = AVL_find(ipaddr, mom_port, ipaddrs)) != NULL)
    {
    lock_node(node, __func__, "AVL_find", LOGLEVEL);
    } /* END if AVL_find != NULL) */
  else if (allow_any_mom)
    {
    char *name = get_cached_nameinfo(addr);

    if (name != NULL)
      snprintf(nodename, sizeof(nodename), "%s", name);
    else if (getnameinfo(&s_addr, len, nodename, sizeof(nodename)-1, NULL, 0, 0) != 0)
      {
      tmpaddr = ntohl(addr->sin_addr.s_addr);
      sprintf(nodename, "0x%lX", tmpaddr);
      }
    else
      insert_addr_name_info(nodename, NULL, addr);

    err = create_partial_pbs_node(nodename, ipaddr, perm);

    if (err == PBSE_NONE)
      {
      node = AVL_find(ipaddr, 0, ipaddrs);
       
      lock_node(node, __func__, "no error", LOGLEVEL);
      }                                                         
    }
    
  if (node == NULL)
    {
    /* node not listed in trusted ipaddrs list */
    
    snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
      "bad attempt to connect from %s (address not trusted - check entry in server_priv/nodes)",
      netaddr(addr));
    
    if (LOGLEVEL >= 2)
      {
      log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
      }
    else
      {
      log_err(-1, __func__, log_buf);
      }
    
    close_conn(chan->sock, FALSE);
    return PBSE_SOCKET_CLOSE;
    }

  if (LOGLEVEL >= 3)
    {
    snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
      "message %s (%d) received from mom on host %s (%s) (sock %d)",
      PBSServerCmds2[command],
      command,
      node->nd_name,
      netaddr(addr),
      chan->sock);

    log_event(PBSEVENT_ADMIN,PBS_EVENTCLASS_SERVER,__func__,log_buf);
    }

  switch (command)
    {
    case IS_NULL:  /* a ping from server */

      DBPRT(("%s: IS_NULL\n", __func__))

      break;

    case IS_UPDATE:

      DBPRT(("%s: IS_UPDATE\n", __func__))

      i = disrui(chan, &ret);

      if (ret != DIS_SUCCESS)
        {
        if (LOGLEVEL >= 1)
          {
          snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
              "IS_UPDATE error %d on node %s\n", ret, node->nd_name);

          log_err(ret, __func__, log_buf);
          }

        goto err;
        }

      DBPRT(("%s: IS_UPDATE %s 0x%x\n", __func__, node->nd_name, i))

      update_node_state(node, i);

      break;

    case IS_STATUS:

      if (LOGLEVEL >= 2)
        {
        snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
            "IS_STATUS received from %s", node->nd_name);

        log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, log_buf);
        }

      if ((node_name = strdup(node->nd_name)) == NULL)
        goto err;
      unlock_node(node, __func__, "before is_stat_get", LOGLEVEL);

      ret = is_stat_get(node_name, chan);
/*      socket_read_flush(chan->sock); */

      node = find_nodebyname(node_name);

      if (ret == SEND_HELLO)
        {
        struct hello_info *hi = calloc(1, sizeof(struct hello_info));
        write_tcp_reply(chan, IS_PROTOCOL, IS_PROTOCOL_VER, IS_STATUS, DIS_SUCCESS);

        hi->name = strdup(node_name);
        enqueue_threadpool_request(send_hierarchy_threadtask, hi);
        ret = DIS_SUCCESS;
        }
      else
        write_tcp_reply(chan,IS_PROTOCOL,IS_PROTOCOL_VER,IS_STATUS,ret);

      node->nd_stream = -1;

      if (ret != DIS_SUCCESS)
        {
        if (LOGLEVEL >= 1)
          {
          snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
              "IS_STATUS error %d on node %s", ret, node_name);

          log_err(ret, __func__, log_buf);
          }
        free(node_name);

        goto err;
        }
      free(node_name);

      break;

    default:

      snprintf(log_buf, LOCAL_LOG_BUF_SIZE,
          "unknown command %d sent from %s",
        command,
        node->nd_name);

      log_err(-1, __func__, log_buf);

      goto err;

      break;
    }  /* END switch (command) */

  /* must be closed because mom opens and closes this connection each time */
  close_conn(chan->sock, FALSE);

  unlock_node(node, __func__, "close", LOGLEVEL);
  
  return PBSE_SOCKET_CLOSE;

err:

  /* a DIS write error has occurred */

  if (node != NULL)
    {
    if (LOGLEVEL >= 1)
      {
      DBPRT(("%s: error processing node %s\n",
            __func__,
            node->nd_name))
      }

    sprintf(log_buf, "%s from %s(%s)",
      dis_emsg[ret],
      node->nd_name,
      netaddr(addr));
    
    unlock_node(node, __func__, "err", LOGLEVEL);
    }
  else
    {
    sprintf(log_buf,"%s occurred when trying to read sock %d",
      dis_emsg[ret],
      chan->sock);
    }
    
  log_err(-1, __func__, log_buf);
    
  close_conn(chan->sock, FALSE);

  return(PBSE_INTERNAL);
  } /* END svr_is_request */




void *write_node_state_work(

  void *vp)

  {
  struct pbsnode *np;
  static char    *fmt = "%s %d\n";
  static FILE    *nstatef = NULL;
  int             iter = -1;

  int             savemask;

  pthread_mutex_lock(node_state_mutex);

  if (LOGLEVEL >= 5)
    {
    DBPRT(("write_node_state_work: entered\n"))
    }

  /* don't store volatile states like down and unknown */

  savemask = INUSE_OFFLINE | INUSE_RESERVE;

  if (nstatef != NULL)
    {
    fseek(nstatef, 0L, SEEK_SET); /* rewind and clear */

    if (ftruncate(fileno(nstatef), (off_t)0) != 0)
      {
      log_err(errno, __func__, "could not truncate file");

      pthread_mutex_unlock(node_state_mutex);
      
      return(NULL);
      }
    }
  else
    {
    /* need to open for first time, temporary-move to pbsd_init */

    if ((nstatef = fopen(path_nodestate, "w+")) == NULL)
      {
      log_err(errno, __func__, "could not open file");

      pthread_mutex_unlock(node_state_mutex);
      
      return(NULL);
      }
    }

  /*
  ** The only state that carries forward is if the
  ** node has been marked offline.
  */

  while ((np = next_host(&allnodes,&iter,NULL)) != NULL)
    {
    if (np->nd_state & INUSE_OFFLINE)
      {
      fprintf(nstatef, fmt, np->nd_name, np->nd_state & savemask);
      }

    unlock_node(np, __func__, NULL, LOGLEVEL);
    } /* END for each node */

  if (fflush(nstatef) != 0)
    {
    log_err(errno, __func__, "failed saving node state to disk");
    }

  fclose(nstatef);
  nstatef = NULL;

  pthread_mutex_unlock(node_state_mutex);

  return(NULL);
  } /* END write_node_state_work() */





void write_node_state(void)

  {
  int rc = enqueue_threadpool_request(write_node_state_work,NULL);

  if (rc)
    {
    log_err(rc, __func__, "Unable to enqueue write_node_state_work task into the threadpool");
    }
  }  /* END write_node_state() */



/* Create a new node_note file then overwrite the previous one.
 *
 *   The note file could get up to:
 *      (# of nodes) * (2 + MAX_NODE_NAME + MAX_NOTE)  bytes in size
 */
int write_node_note(void)

  {
  struct pbsnode *np;
  int             iter = -1;
  FILE           *nin;

  if (LOGLEVEL >= 2)
    {
    DBPRT(("%s: entered\n", __func__))
    }

  if ((nin = fopen(path_nodenote_new, "w")) == NULL)
    goto err1;

  if ((svr_totnodes == 0))
    {
    log_event(
      PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, "Server has empty nodes list");

    fclose(nin);

    return(-1);
    }

  /* for each node ... */
  while ((np = next_host(&allnodes, &iter, NULL)) != NULL)
    {
    /* write node name followed by its note string */
    if ((np->nd_note != NULL) && 
        (np->nd_note[0] != '\0'))
      {
      fprintf(nin, "%s %s\n", np->nd_name, np->nd_note);
      }
    
    unlock_node(np, __func__, NULL, LOGLEVEL);
    }

  fflush(nin);

  if (ferror(nin))
    {
    fclose(nin);
    goto err1;
    }

  fclose(nin);

  if (rename(path_nodenote_new, path_nodenote) != 0)
    {
    log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__,
      "replacing old node note file failed");

    return(-1);
    }

  return(PBSE_NONE);

err1:
  log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__,
    "Node note file update failed");

  return(-1);
  }  /* END write_node_note() */



/*
 * free_prop - free list of prop structures created by proplist()
 */

static void free_prop(

  struct prop *prop)

  {
  struct prop *pp;

  for (pp = prop;pp != NULL;pp = prop)
    {
    prop = pp->next;

    free(pp->name);
    free(pp);
    }  /* END for (pp) */

  return;
  }    /* END free_prop() */




void *node_unreserve_work(

  void *vp)

  {
  resource_t       handle = *((resource_t *)vp);

  struct  pbsnode *np;
  struct  pbssubn *sp;
  int              iter = -1;

  /* clear old reserve */
  while ((np = next_host(&allnodes,&iter,NULL)) != NULL)
    {
    for (sp = np->nd_psn;sp;sp = sp->next)
      {
      if (sp->inuse & INUSE_RESERVE)
        {
        if ((handle == RESOURCE_T_ALL) || (handle == sp->allocto))
          {
          np->nd_nsnfree++;
          
          sp->inuse    &= ~INUSE_RESERVE;
          np->nd_state &= ~INUSE_RESERVE;
          }
        }
      }

    unlock_node(np, "node_unreserve_work", NULL, LOGLEVEL);
    }

  return(NULL);
  } /* END node_unreserve_work() */





/*
 * unreserve - unreserve nodes
 *
 * If handle is set to a existing resource_t, then release all nodes
 * associated with that handle, otherwise, (this is dangerous)
 * if handle == RESOURCE_T_ALL, release all nodes period.
 */

void node_unreserve(

  resource_t handle)

  {
  int rc = enqueue_threadpool_request(node_unreserve_work,NULL);

  if (rc)
    {
    log_err(rc, __func__, "Unable to enqueue node_unreserve task into the threadpool");
    }
  }  /* END node_unreserve() */




/*
** Look through the property list and make sure that all
** those marked are contained in the node.
*/

int hasprop(

  struct pbsnode *pnode,
  struct prop    *props)

  {
  struct  prop    *need;

  for (need = props;need;need = need->next)
    {

    struct prop *pp;

    if (need->mark == 0) /* not marked, skip */
      continue;

    for (pp = pnode->nd_first;pp != NULL;pp = pp->next)
      {
      if (strcmp(pp->name, need->name) == 0)
        break;  /* found it */
      }

    if (pp == NULL)
      {
      return(0);
      }
    }

  return(1);
  }  /* END hasprop() */





/*
 * see if node has the number of processors required
 * if free == SKIP_NONE,  check against total number of processors, else
 * if free != SKIP_NONE,  check against number free
 *
 * Return 1 if possible, 0 if not
 */

static int hasppn(

  struct pbsnode *pnode,     /* I */
  int             node_req,  /* I */
  int             free)      /* I */

  {
  if ((free != SKIP_NONE) &&
      (free != SKIP_NONE_REUSE) &&
      (pnode->nd_nsnfree >= node_req))
    {
    return(1);
    }

  if ((free == SKIP_NONE) && 
      (pnode->nd_nsn >= node_req))
    {
    return(1);
    }

  return(0);
  }  /* END hasppn() */




/*
** Count how many gpus are available for use on this node
*/
static int gpu_count(

  struct pbsnode *pnode,  /* I */
  int    freeonly)        /* I */

  {
  int  count = 0;
  char log_buf[LOCAL_LOG_BUF_SIZE];

  if ((pnode->nd_state & INUSE_OFFLINE) ||
      (pnode->nd_state & INUSE_UNKNOWN) ||
      (pnode->nd_state & INUSE_DOWN))
    {
    if (LOGLEVEL >= 7)
      {
      sprintf(log_buf,
        "Counted %d gpus %s on node %s that was skipped",
        count,
        (freeonly? "free":"available"),
        pnode->nd_name);
    
      log_ext(-1, __func__, log_buf, LOG_DEBUG);
      }
    return (count);
    }

#ifdef NVIDIA_GPUS
  if (pnode->nd_gpus_real)
    {
    int j;

    for (j = 0; j < pnode->nd_ngpus; j++)
      {
      struct gpusubn *gn = pnode->nd_gpusn + j;

      /* always ignore unavailable gpus */
      if (gn->state == gpu_unavailable)
        continue;

      if (!freeonly)
        {
        count++;
        }
      else if ((gn->state == gpu_unallocated) ||
            ((gn->state == gpu_shared) && (gpu_mode_rqstd == gpu_normal)))
        {
        count++;;
        }
      }
    }
  else
#endif  /* NVIDIA_GPUS */
    {
    /* virtual gpus */
    if (freeonly)
      {
      count = pnode->nd_ngpus_free;
      }
    else
      {
      count = pnode->nd_ngpus;
      }
    }

  if (LOGLEVEL >= 7)
    {
    sprintf(log_buf,
      "Counted %d gpus %s on node %s",
      count,
      (freeonly? "free":"available"),
      pnode->nd_name);

    log_ext(-1, __func__, log_buf, LOG_DEBUG);
    }

  return(count);
  }  /* END gpu_count() */





#ifdef NVIDIA_GPUS
/*
** get gpu index for this gpuid
*/
int gpu_entry_by_id(

  struct pbsnode *pnode,  /* I */
  char   *gpuid,
  int    get_empty)

  {
  if (pnode->nd_gpus_real)
    {
    int j;

    for (j = 0; j < pnode->nd_ngpus; j++)
      {
      struct gpusubn *gn = pnode->nd_gpusn + j;

      if ((gn->gpuid != NULL) && (strcmp(gpuid, gn->gpuid) == 0))
        {
        return(j);
        }
      }
    }

  /*
   * we did not find the entry.  if get_empty is set then look for an empty
   * slot.  If none is found then try to add a new entry to nd_gpusn
   */

  if (get_empty)
    {
    int j;

    for (j = 0; j < pnode->nd_ngpus; j++)
      {
      struct gpusubn *gn = pnode->nd_gpusn + j;

      if (gn->gpuid == NULL)
        {
        return(j);
        }
      }

    create_a_gpusubnode(pnode);
    return (pnode->nd_ngpus - 1);    
    }

  return (-1);
  }  /* END gpu_entry_by_id() */
#endif  /* NVIDIA_GPUS */





/*
 * checks if a node is ok for to reshuffle
 *
 * All parameters are exactly the same as search
 * @param pnode - the node we're looking at
 *
 * @return TRUE if the node is reshuffleable for search's purposes
 */
int can_reshuffle(

  struct pbsnode *pnode,
  struct prop    *glorf,
  int             skip,
  int             vpreq,
  int             gpureq,
  int             pass)

  {
  char log_buf[LOCAL_LOG_BUF_SIZE];

  if (pnode->nd_ntype == NTYPE_CLUSTER)
    {
    if (pnode->nd_flag != thinking)
      {
      /* only shuffle nodes which have been selected above */

      return(FALSE);
      }

    if (pnode->nd_state & pass)
      return(FALSE);

    if (LOGLEVEL >= 6)
      {
      sprintf(log_buf,
        "search(2): starting eval gpus on node %s need %d(%d) mode %d has %d free %d skip %d",
        pnode->nd_name,
        gpureq,
        pnode->nd_ngpus_needed,
        gpu_mode_rqstd,
        pnode->nd_ngpus,
        gpu_count(pnode, TRUE),
        skip);

       log_ext(-1, __func__, log_buf, LOG_DEBUG);
       }

    if ((skip == SKIP_EXCLUSIVE) && 
        (vpreq < pnode->nd_nsnfree) &&
        (gpureq < gpu_count(pnode, TRUE)))
      return(FALSE);

    if ((skip == SKIP_ANYINUSE) &&
        (vpreq < (pnode->nd_nsnfree + pnode->nd_nsnshared)) &&
        (gpureq < gpu_count(pnode, TRUE)))
      return(FALSE);

    if (!hasprop(pnode, glorf))
      return(FALSE);
    }
  else
    return(FALSE);

  return(TRUE);
  } /* can_reshuffle() */





/*
** Parse a number in a spec.
** Return 0 if okay, 1 if no number exists, -1 on error
*/

static int number(

  char **ptr,
  int   *num)

  {
  char  holder[80];
  int   i = 0;
  char *str = *ptr;
  char  log_buf[LOCAL_LOG_BUF_SIZE];

  while (isdigit(*str) && (unsigned int)(i + 1) < sizeof holder)
    holder[i++] = *str++;

  if (i == 0)
    {
    return(1);
    }

  holder[i] = '\0';

  if ((i = atoi(holder)) <= 0)
    {
    sprintf(log_buf, "zero illegal");

    return(-1);
    }

  *ptr = str;

  *num = i;

  return(0);
  }  /* END number() */




/*
** Check string to see if it is a legal property name.
** If not, return 1.
** *prop set to static char array containing the properity,
** must be copied.
*/

static int property(

  char **ptr,
  char **prop)

  {
  char        *str = *ptr;
  char        *dest = *prop;
  int          i = 0;
  char         log_buf[LOCAL_LOG_BUF_SIZE];
  long         cray_enabled = FALSE;

  get_svr_attr_l(SRV_ATR_CrayEnabled, &cray_enabled);

  if (!isalpha(*str))
    {
    if ((cray_enabled == FALSE) ||
        (is_compute_node(str) == FALSE))
      {
      sprintf(log_buf, "first character of property (%s) not a letter", str);
      
      return(1);
      }
    }

  while (isalnum(*str) || *str == '-' || *str == '.' || *str == '=' || *str == '_')
    dest[i++] = *str++;

  dest[i] = '\0';

  /* skip over "/vp_number" */

  if (*str == '/')
    {
    do
      {
      str++;
      }
    while (isdigit(*str));
    }

  *ptr = str;

  return(0);
  }  /* END property() */





/*
** Create a property list from a string.
** Return 0 if all is well, 1 otherwise.
*/

int proplist(

  char        **str,
  struct prop **plist,
  int          *node_req,
  int          *gpu_req)

  {
  struct prop *pp;
  char         name_storage[80];
  char        *pname;
  char        *pequal;
#ifdef NVIDIA_GPUS
  int          have_gpus = FALSE;
  char         log_buf[LOCAL_LOG_BUF_SIZE];
#endif  /* NVIDIA_GPUS */

  *node_req = 1; /* default to 1 processor per node */

  pname  = name_storage;
  *pname = '\0';

  for (;;)
    {
    if (property(str, &pname))
      {
      return(1);
      }

    if (*pname == '\0')
      break;

    if ((pequal = strchr(pname, (int)'=')) != NULL)
      {
      /* special property */

      /* identify the special property and place its value */
      /* into node_req       */

      *pequal = '\0';

      if (strcmp(pname, "ppn") == 0)
        {
        pequal++;

        if ((number(&pequal, node_req) != 0) || (*pequal != '\0'))
          {
          return(1);
          }
        }
      else if (strcmp(pname, "gpus") == 0)
        {
        pequal++;

        if ((number(&pequal, gpu_req) != 0) || (*pequal != '\0'))
          {
          return(1);
          }
#ifdef NVIDIA_GPUS
        have_gpus = TRUE;
        gpu_err_reset = FALSE; /* default to no */
#endif  /* NVIDIA_GPUS */

        /* default value if no other gets specified */

        gpu_mode_rqstd = gpu_exclusive_thread;
        }
      else
        {
        return(1); /* not recognized - error */
        }
      }
#ifdef NVIDIA_GPUS
    else if (have_gpus && (!strcasecmp(pname, "exclusive_thread")))
      {
      gpu_mode_rqstd = gpu_exclusive_thread;
      }
    else if (have_gpus && (!strcasecmp(pname, "exclusive")))
      {
      gpu_mode_rqstd = gpu_exclusive_thread;
      }
    else if (have_gpus && (!strcasecmp(pname, "exclusive_process")))
      {
      gpu_mode_rqstd = gpu_exclusive_process;
      }
    else if (have_gpus && (!strcasecmp(pname, "default")))
      {
      gpu_mode_rqstd = gpu_normal;
      }
    else if (have_gpus && (!strcasecmp(pname, shared)))
      {
      gpu_mode_rqstd = gpu_normal;
      }
    else if (have_gpus && (!strcasecmp(pname, "reseterr")))
      {
      gpu_err_reset = TRUE;
      }
#endif  /* NVIDIA_GPUS */
    else
      {
      pp = (struct prop *)calloc(1, sizeof(struct prop));

      pp->mark = 1;
      pp->name = strdup(pname);
      pp->next = *plist;

      *plist = pp;
      }

#ifdef NVIDIA_GPUS
    if ((have_gpus) && (LOGLEVEL >= 7))
      {
      sprintf(log_buf,
        "proplist: set needed gpu mode to %d",
        gpu_mode_rqstd);

       log_ext(-1, __func__, log_buf, LOG_DEBUG);
      }
#endif  /* NVIDIA_GPUS */

    if (**str != ':')
      break;

    (*str)++;
    }  /* END for(;;) */

  return(PBSE_NONE);
  }  /* END proplist() */




/*
** Add the "global" spec to every sub-spec in "spec".
**      RETURNS:  allocated string buffer (must be freed externally)
*/

static char *mod_spec(

  char *spec,    /* I */
  char *global)  /* I */

  {
  char  *line;
  char  *cp;
  int    len;
  int    nsubspec;

  nsubspec = 1;

  for (cp = spec;*cp != '\0';cp++)
    {
    if (*cp == '+')
      {
      nsubspec++;
      }
    }

  len = strlen(global);

  line = calloc(1, nsubspec * (len + 1) + strlen(spec) + 1);

  if (line == NULL)
    {
    /* FAILURE */

    return(NULL);
    }

  cp = line;

  while (*spec)
    {
    if (*spec == '+')
      {
      *cp++ = ':';

      strcpy(cp, global);

      cp += len;
      }

    *cp++ = *spec++;
    }

  *cp++ = ':';

  strcpy(cp, global);

  return(line);
  }  /* END mod_spec() */





int MSNPrintF(

  char **BPtr,   /* I */
  int   *BSpace, /* I */
  char  *Format, /* I */
  ...)           /* I */

  {
  int len;

  va_list Args;

  if ((BPtr == NULL) ||
      (BSpace == NULL) ||
      (Format == NULL) ||
      (*BSpace <= 0))
    {
    return(FAILURE);
    }

  va_start(Args,Format);

  len = vsnprintf(*BPtr,*BSpace,Format,Args);

  va_end(Args);

  if (len <= 0)
    {
    return(FAILURE);
    }
  
  if (len >= *BSpace)
    {
    /* truncation occurred due to attempted
     * overflow! */

    /* do not place BPtr past the end of the buffer:
     * it is too dangerous (calling function could derference it
     * to check for empty string, etc.)! */

    *BPtr += (*BSpace) - 1;
    *BSpace = 0;

    return(FAILURE);
    }
  
  *BPtr += len;
  *BSpace -= len;

  return(SUCCESS);
  }  /* END MSNPrintF() */


/*
 * Test a procs specification.
 *
 * Return >0 - number of procs counted in the spec if it works,
 *         0 - if it cannot be satisfied now,
 *        -1 - if it can never be satisfied.
 *
 */
int procs_available(
    
  int proc_ct)

  {
  int             iter = -1;
  int             procs_avail = 0;
  struct pbsnode *pnode;

  if (proc_ct > svr_clnodes)
    {
    /* user requested more processors than are available on the system*/
    return(-1);
    }

  while ((pnode = next_host(&allnodes,&iter,NULL)) != NULL)
    {
    procs_avail += pnode->nd_nsnfree;

    unlock_node(pnode, "procs_available", NULL, LOGLEVEL);
    }

  if (proc_ct > procs_avail)
    {
    return(0);
    }

  return(procs_avail);
  } /* END procs_available() */




int node_is_spec_acceptable(

  struct pbsnode   *pnode,
  single_spec_data *spec,
  char             *ProcBMStr,
  int               exclusive,
  int              *eligible_nodes)

  {
  struct pbssubn *snp;
  struct prop    *prop = spec->prop;

  int             ppn_req = spec->ppn;
  int             gpu_req = spec->gpu;

#ifdef GEOMETRY_REQUESTS
  if (IS_VALID_STR(ProcBMStr))
    {
    if (pnode->nd_state != INUSE_FREE)
      return(FALSE);

    if (node_satisfies_request(pnode, ProcBMStr) == FALSE)
      return(FALSE);
    }
#endif

  /* NYI: check if these are necessary */
  pnode->nd_flag = okay;

  for (snp = pnode->nd_psn; snp != NULL; snp = snp->next)
    {
    snp->flag = okay;

    if (LOGLEVEL >= 9)
      DBPRT(("%s: %s/%d inuse 0x%x nprops %d\n",
        __func__,
        pnode->nd_name,
        snp->index,
        snp->inuse,
        pnode->nd_nprops))
    }

  if (pnode->nd_ntype != NTYPE_CLUSTER)
    return(FALSE);

  /* make sure that the node has properties */
  if (hasprop(pnode, prop) == FALSE)
    return(FALSE);

  if ((hasppn(pnode, ppn_req, SKIP_NONE) == FALSE) ||
      (gpu_count(pnode, FALSE) < gpu_req))
    return(FALSE);

  (*eligible_nodes)++;

  if ((pnode->nd_state & (INUSE_OFFLINE | INUSE_DOWN | INUSE_RESERVE | INUSE_JOB)) != 0)
    return(FALSE);

  if (exclusive == TRUE) 
    {
    int gpu_free = gpu_count(pnode, TRUE) - pnode->nd_ngpus_to_be_used;
    int np_free  = pnode->nd_nsnfree - pnode->nd_np_to_be_used;

    if ((ppn_req > np_free) ||
        (gpu_req > gpu_free))
      return(FALSE);
    }

  return(TRUE);
  } /* END node_is_spec_acceptable() */




int parse_req_data(
    
  complete_spec_data *all_reqs)

  {
  int               i;
  int               j = 0;
  long              cray_enabled = FALSE;

  single_spec_data *req;

  get_svr_attr_l(SRV_ATR_CrayEnabled, &cray_enabled);
  all_reqs->total_nodes = 0;

  for (i = 0; i < all_reqs->num_reqs; i++)
    {
    req = all_reqs->reqs + i;
    req->nodes = 1;
    req->gpu   = 0;
    req->ppn   = 1;
    req->prop  = NULL;

    if ((cray_enabled == FALSE) ||
        (is_compute_node(all_reqs->req_start[i]) == FALSE))
      {
      if ((j = number(&(all_reqs->req_start[i]), &(req->nodes))) == -1)
        return(j);
      }

    if (j == 0)
      {
      /* there was a number */
      if (*(all_reqs->req_start[i]) != '\0')
        {
        if (*(all_reqs->req_start[i]) == ':')
          all_reqs->req_start[i]++;
        
        if (proplist(&(all_reqs->req_start[i]), &(req->prop), &(req->ppn), &(req->gpu)))
          return(-1);
        }
      }
    else
      {
      if (*(all_reqs->req_start[i]) != '\0')
        {
        if (proplist(&(all_reqs->req_start[i]), &(req->prop), &(req->ppn), &(req->gpu)))
          return(-1);
        }
      }

    all_reqs->total_nodes += req->nodes;
    }

  return(PBSE_NONE);
  } /* END parse_req_data() */




/* 
 * builds the node_job_add_info struct that will be used by set_nodes
 * instead of looping over different nodes.
 */

int save_node_for_adding(
    
  node_job_add_info *naji,
  struct pbsnode    *pnode,
  single_spec_data  *req,
  char              *first_node_name,
  int                is_external_node)

  {
  node_job_add_info *to_add;
  node_job_add_info *old_next;

  if ((first_node_name[0] != '\0') &&
      (!strcmp(first_node_name, pnode->nd_name)))
    pnode->nd_order = 0;
  else
    pnode->nd_order = 1;

  if (naji->node_name[0] == '\0')
    {
    /* first */
    strcpy(naji->node_name, pnode->nd_name);
    naji->ppn_needed = req->ppn;
    naji->gpu_needed = req->gpu;
    naji->is_external = is_external_node;
    }
  else
    {
    /* second */
    if ((to_add = calloc(1, sizeof(node_job_add_info))) == NULL)
      {
      log_err(ENOMEM, __func__, "Cannot allocate memory!");

      return(ENOMEM);
      }
    
    /* initialize to_add */
    strcpy(to_add->node_name, pnode->nd_name);
    to_add->ppn_needed = req->ppn;
    to_add->gpu_needed = req->gpu;
    to_add->is_external = is_external_node;

    /* fix pointers, NOTE: works even if old_next == NULL */
    old_next = naji->next;
    to_add->next = old_next;
    naji->next = to_add;
    }

  /* count off the number we have reserved */
  pnode->nd_np_to_be_used    += req->ppn;
  pnode->nd_ngpus_to_be_used += req->gpu;

  return(PBSE_NONE);
  } /* END save_node_for_adding */




/*
 * if there is a node being requested, the spec should look like
 * node_name[:ppn=X][+]...
 * otherwise it should look like:
 * <NUM_NODES>[:ppn=X][+]...
 *
 * If a specific node is being requested first, copy just the
 * name into first_node_name  
 */

void set_first_node_name(
    
  char *spec_param,      /* I */
  char *first_node_name) /* O */

  {
  int   i;
  int   len;

  if (isdigit(spec_param[0]) == TRUE)
    {
    first_node_name[0] = '\0';
    }
  else
    {
    len = strlen(spec_param);
    
    for (i = 0; i < len; i++)
      {
      /* a ':' means you've moved on to ppn and a + means its the next req */
      if ((spec_param[i] == ':') ||
          (spec_param[i] == '+'))
        break;
      else
        first_node_name[i] = spec_param[i];
      }
    
    /* make sure you NULL terminate */
    first_node_name[i] = '\0';
    }

  } /* END set_first_node_name() */




int is_reserved_property(

  char *prop)

  {
  if ((strncmp(prop, "ppn", strlen("ppn")) == 0) ||
      (strncmp(prop, "gpus", strlen("gpus") == 0)) ||
      (strncasecmp(prop, "exclusive_thread", strlen("exclusive_thread")) == 0) ||
      (strncasecmp(prop, "exclusive", strlen("exclusive")) == 0) ||
      (strncasecmp(prop, "exclusive_process", strlen("exclusive_process")) == 0) ||
      (strncasecmp(prop, "default", strlen("default")) == 0) ||
      (strncasecmp(prop, "shared", strlen("shared")) == 0) ||
      (strncasecmp(prop, "reseterr", strlen("reseterr")) == 0))
    return(TRUE);
  else
    return(FALSE);
  } /* END is_reserved_property() */




int is_compute_node(

  char *node_id)

  {
  struct pbsnode *pnode;
  int             rc = FALSE;
  char           *colon;
  char           *plus;

  if ((colon = strchr(node_id, ':')) != NULL)
    {
    if ((!strcmp(colon + 1, "external")) ||
        (!strcmp(colon + 1, alps_reporter_feature)) || 
        (!strcmp(colon + 1, alps_starter_feature)))
      {
      return(rc);
      }
    else
      *colon = '\0';
    }
  
  if ((plus = strchr(node_id, '+')) != NULL)
    *plus = '\0';

  if ((pnode = find_nodebyname(node_id)) != NULL)
    {
    rc = TRUE;
    unlock_node(pnode, __func__, NULL, 0);
    }

  if (colon != NULL)
    *colon = ':';

  if (plus != NULL)
    *plus = '+';

  return(rc);
  } /* END is_compute_node() */




void release_node_allocation(
    
  node_job_add_info *naji)

  {
  node_job_add_info *current = NULL;
  struct pbsnode    *pnode = NULL;

  current = naji;
  while (current != NULL) 
    {
    if ((pnode = find_nodebyname(current->node_name)) != NULL)
      {
      pnode->nd_np_to_be_used    -= current->ppn_needed;
      pnode->nd_ngpus_to_be_used -= current->gpu_needed;
      unlock_node(pnode, __func__, NULL, LOGLEVEL);
      }
    current = current->next;
    }
  } /* END release_node_allocation() */




int check_for_node_type(

  complete_spec_data *all_reqs,
  enum node_types     nt)

  {
  single_spec_data *req;
  int               i;
  int               found_type = FALSE;
  struct pbsnode   *pnode;
  struct pbsnode   *reporter = alps_reporter;
  struct prop      *p;

  if (reporter == NULL)
    {
    /* this shouldn't be possible */
    log_err(-1, __func__, "Checking for node types with a non-cray enabled pbs_server??");
    return(-1);
    }

  lock_node(reporter, __func__, NULL, 0);

  for (i = 0; i < all_reqs->num_reqs; i++)
    {
    req = all_reqs->reqs + i;

    for (p = req->prop; p != NULL; p = p->next)
      {
      if ((!strcmp(p->name, "cray_compute")) ||
          (!strcmp(p->name, alps_starter_feature)))
        continue;

      pnode = find_node_in_allnodes(&(reporter->alps_subnodes), p->name);

      if (pnode != NULL)
        {
        unlock_node(pnode, __func__, NULL, 0);

        if (nt == ND_TYPE_CRAY)
          {
          found_type = TRUE;
  
          break;
          }
        }
      else if (nt != ND_TYPE_CRAY)
        {
        int login = FALSE;

        unlock_node(reporter, __func__, NULL, 0);
        pnode = find_nodebyname(p->name);
        lock_node(reporter, __func__, NULL, 0);

        if (pnode != NULL)
          {
          if (pnode->nd_is_alps_login == TRUE)
            login = TRUE;

          unlock_node(pnode, __func__, NULL, 0);

          if (nt == ND_TYPE_EXTERNAL)
            {
            if (login == FALSE)
              found_type = TRUE;
            }
          else if (nt == ND_TYPE_LOGIN)
            if (login == TRUE)
              found_type = TRUE;

          break;
          }
        }
      }

    if (found_type == TRUE)
      break;
    }
  
  unlock_node(reporter, __func__, NULL, 0);

  return(found_type);
  } /* END check_for_node_type() */




enum job_types find_job_type(

  complete_spec_data *all_reqs)

  {
  enum job_types jt = JOB_TYPE_login;
  
  if (check_for_node_type(all_reqs, ND_TYPE_CRAY) == TRUE)
    {
    if (check_for_node_type(all_reqs, ND_TYPE_EXTERNAL) == TRUE)
      jt = JOB_TYPE_heterogeneous;
    else
      jt = JOB_TYPE_cray;
    }
  else if (check_for_node_type(all_reqs, ND_TYPE_EXTERNAL) == TRUE)
    {
    jt = JOB_TYPE_normal;
    }
  else if (check_for_node_type(all_reqs, ND_TYPE_LOGIN) == TRUE)
    jt = JOB_TYPE_login;

  return(jt);
  } /* END find_job_type() */




int add_login_node_if_needed(

  char              **first_node_name_ptr,
  char               *login_prop,
  node_job_add_info  *naji)

  {
  char             *first_node_name = *first_node_name_ptr;
  struct pbsnode   *login = find_nodebyname(first_node_name);
  int               need_to_add_login = FALSE;
  int               rc = PBSE_NONE;
  int               dummy1;
  int               dummy2;
  struct prop      *prop = NULL;
  single_spec_data  req;

  if (login == NULL)
    need_to_add_login = TRUE;
  else
    {
    if (login->nd_is_alps_login == FALSE)
      need_to_add_login = TRUE;

    unlock_node(login, __func__, NULL, 0);
    }

  if (need_to_add_login == TRUE)
    {
    if (login_prop != NULL)
      {
      proplist(&login_prop, &prop, &dummy1, &dummy2);
      }

    if ((login = get_next_login_node(prop)) == NULL)
      rc = -1;
    else
      {
      if (naji != NULL)
        {
        /* add to naji */
        req.nodes = 1;
        req.ppn = 1;
        req.gpu = 0;
        req.prop = NULL;
        save_node_for_adding(naji, login, &req, login->nd_name, FALSE);
        strcpy(*first_node_name_ptr, login->nd_name);
        }
      
      rc = PBSE_NONE;

      unlock_node(login, __func__, NULL, 0);
      }

    if (prop != NULL)
      free_prop(prop);
    }

  return(rc);
  } /* END add_login_node_if_needed() */




int node_is_external(

  struct pbsnode *pnode)

  {
  int is_external = FALSE;

  /* all logins have nd_is_alps_login set to true.
   * all cray computes have their parent pointer set to alps_reporter.
   * if neither of these are found, it must be an external node */
  if ((pnode->nd_is_alps_login == FALSE) &&
      (pnode->parent == NULL))
    is_external = TRUE;
  
  return(is_external);
  } /* END node_is_external() */




/*
 * Test a node specification.
 *
 * Return >0 - number of nodes counted in the spec if it works,
 *         0 - if it cannot be satisfied,
 *        -1 - if it can never be satisfied.
 * Okay to bail early if "early" is true.
 * VPs selected are marked "thinking"
 */

int node_spec(

  char               *spec_param, /* I */
  int                 early,      /* I (boolean) */
  int                 exactmatch, /* I (boolean) - NOT USED */
  char               *ProcBMStr,  /* I */
  char               *FailNode,   /* O (optional,minsize=1024) */
  node_job_add_info  *naji,       /* O (optional) */
  char               *EMsg,       /* O (optional,minsize=1024) */
  char               *login_prop, /* I (optional) */
  alps_req_data     **ard_array,  /* O (optional) */
  int                *num_reqs)   /* O (optional) */

  {
  struct pbsnode      *pnode;
  char                 first_node_name[PBS_MAXHOSTNAME + 1];
  char                *first_name_ptr;
  node_iterator        iter;
  char                 log_buf[LOCAL_LOG_BUF_SIZE];

  char                *globs;
  char                *cp;
  char                *hold;
  int                  i;
  int                  num;
  int                  rc;
  int                  eligible_nodes = 0;
  complete_spec_data   all_reqs;
  char                *spec;
  char                *plus;
  long                 cray_enabled = FALSE;
  enum job_types       job_type = JOB_TYPE_normal;
  int                  num_alps_reqs = 0;

  if (EMsg != NULL)
    EMsg[0] = '\0';

  if (FailNode != NULL)
    FailNode[0] = '\0';

  if (LOGLEVEL >= 6)
    {
    sprintf(log_buf, "entered spec=%.4000s", spec_param);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);

    DBPRT(("%s\n", log_buf));
    }

  exclusive = 1; /* by default, nodes (VPs) are requested exclusively */

  set_first_node_name(spec_param, first_node_name);
  get_svr_attr_l(SRV_ATR_CrayEnabled, &cray_enabled);

  spec = strdup(spec_param);

  if (spec == NULL)
    {
    /* FAILURE */
    sprintf(log_buf, "cannot alloc memory");

    if (LOGLEVEL >= 1)
      {
      log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
      }

    if (EMsg != NULL)
      {
      snprintf(EMsg, 1024, "%s", log_buf);
      }

    return(-1);
    }

  if ((globs = strchr(spec, '#')) != NULL)
    {
    *globs++ = '\0';

    globs = strdup(globs);

    while ((cp = strrchr(globs, '#')) != NULL)
      {
      *cp++ = '\0';

      if (strcmp(cp, shared) != 0)
        {
        hold = mod_spec(spec, cp);

        free(spec);

        spec = hold;
        }
      else
        {
        exclusive = 0;
        }
      }

    if (strcmp(globs, shared) != 0)
      {
      hold = mod_spec(spec, globs);

      free(spec);

      spec = hold;
      }
    else
      {
      exclusive = 0;
      }

    free(globs);
    }  /* END if ((globs = strchr(spec,'#')) != NULL) */

  all_reqs.num_reqs = 1;
  plus = spec;

  /* count number of reqs */
  while (*plus != '\0')
    {
    if ((*plus == '+') ||
        (*plus == '|'))
      all_reqs.num_reqs++;

    plus++;
    }

  /* allocate space in all_reqs */
  all_reqs.reqs      = calloc(all_reqs.num_reqs, sizeof(single_spec_data));
  all_reqs.req_start = calloc(all_reqs.num_reqs, sizeof(char *));

  if ((all_reqs.reqs == NULL) ||
      (all_reqs.req_start == NULL))
    {
    if (all_reqs.reqs != NULL)
      free(all_reqs.reqs);
    else if (all_reqs.req_start != NULL)
      free(all_reqs.req_start);

    log_err(ENOMEM, __func__, "Cannot allocate memory!");
    free(spec);
    return(-1);
    }

  /* set up pointers for reqs */
  plus = spec;
  i = 0;
  all_reqs.req_start[i] = spec;
  i++;

  while (*plus != '\0')
    {
    /* make the '+' NULL and advance past it */
    if (*plus == '|')
      num_alps_reqs++;

    if ((*plus == '|') ||
        (*plus == '+'))
      {
      all_reqs.reqs[i].req_id = num_alps_reqs;
      
      *plus = '\0';
      plus++;
      
      /* advance past "nodes=" */
      if (!strncmp(plus, "nodes=", strlen("nodes=")))
        plus += strlen("nodes=");
      
      all_reqs.req_start[i] = plus;
      i++;
      }
    else
      plus++;
    }

  /* now parse each spec into the data */
  if ((rc = parse_req_data(&all_reqs)) != PBSE_NONE)
    {
    /* FAILURE */
    for (i = 0; i < all_reqs.num_reqs; i++)
      free_prop(all_reqs.reqs[i].prop);
    
    free(all_reqs.reqs);
    free(all_reqs.req_start);

    free(spec);

    return(rc);
    }

  num = all_reqs.total_nodes;

#ifndef CRAY_MOAB_PASSTHRU
  if (num > svr_clnodes)
    {
    /* FAILURE */

    free(spec);

    sprintf(log_buf, "job allocation request exceeds available cluster nodes, %d requested, %d available",
      num,
      svr_clnodes);

    if (LOGLEVEL >= 6)
      {
      log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
      }

    if (EMsg != NULL)
      {
      snprintf(EMsg, 1024, "%s", log_buf);
      }

    return(-1);
    }
#endif

  if (LOGLEVEL >= 6)
    {
    sprintf(log_buf, "job allocation debug: %d requested, %d svr_clnodes, %d svr_totnodes",
      num,
      svr_clnodes,
      svr_totnodes);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);

    DBPRT(("%s\n", log_buf));
    }

  if (cray_enabled == TRUE)
    {
    job_type = find_job_type(&all_reqs);

    first_name_ptr = first_node_name;

    if ((job_type == JOB_TYPE_cray) ||
        (job_type == JOB_TYPE_heterogeneous))
      {
      if (add_login_node_if_needed(&first_name_ptr, login_prop, naji) != PBSE_NONE)
        {
        snprintf(log_buf, sizeof(log_buf), 
          "Couldn't find an acceptable login node for spec '%s' with feature request '%s'",
          spec_param,
          (login_prop != NULL) ? login_prop : "null");
        
        log_err(-1, __func__, log_buf);

        free(spec);

        return(-1);
        }
      }

    if ((num_alps_reqs > 0) &&
        (ard_array != NULL))
      {
      *ard_array = calloc(num_alps_reqs + 1, sizeof(alps_req_data));
      
      for (i = 0; i <= num_alps_reqs; i++)
        (*ard_array)[i].node_list = get_dynamic_string(-1, NULL);

      *num_reqs = num_alps_reqs + 1;
      }
    }

  reinitialize_node_iterator(&iter);
  pnode = NULL;

  /* iterate over all nodes */
  while ((pnode = next_node(&allnodes,pnode,&iter)) != NULL)
    {
    /* check each req against this node to see if it satisfies it */
    for (i = 0; i < all_reqs.num_reqs; i++)
      {
      single_spec_data *req = all_reqs.reqs + i;

      if (req->nodes > 0)
        {
        if (node_is_spec_acceptable(pnode, req, ProcBMStr, exclusive, &eligible_nodes) == TRUE)
          {
          if (naji != NULL)
            {
            /* for heterogeneous jobs on the cray, record the external 
             * nodes in a separate attribute */
            if ((job_type == JOB_TYPE_heterogeneous) &&
                (node_is_external(pnode) == TRUE))
              save_node_for_adding(naji, pnode, req, first_node_name, TRUE);
            else
              save_node_for_adding(naji, pnode, req, first_node_name, FALSE);

            if ((num_alps_reqs > 0) &&
                (ard_array != NULL) &&
                (*ard_array != NULL))
              {
              if ((*ard_array)[req->req_id].node_list->used != 0)
                append_char_to_dynamic_string((*ard_array)[req->req_id].node_list, ',');

              append_dynamic_string((*ard_array)[req->req_id].node_list, pnode->nd_name);

              if (req->ppn > (*ard_array)[req->req_id].ppn)
                (*ard_array)[req->req_id].ppn = req->ppn;
              }
            }

          /* decrement needed nodes */
          all_reqs.total_nodes--;
          req->nodes--;
    
          /* are all reqs satisfied? */
          if (all_reqs.total_nodes == 0)
            break;
          }
        }
      }

    /* are all reqs satisfied? */
    if (all_reqs.total_nodes == 0)
      {
      unlock_node(pnode, __func__, NULL, LOGLEVEL);
      break;
      }
    } /* END for each node */

  for (i = 0; i < all_reqs.num_reqs; i++)
    if (all_reqs.reqs[i].prop != NULL)
      free_prop(all_reqs.reqs[i].prop);
  
  free(all_reqs.reqs);
  free(all_reqs.req_start);

  free(spec);

#ifndef CRAY_MOAB_PASSTHRU
  if (eligible_nodes < num)
    {
    /* sufficient eligible nodes do not exist */
    /* FAILURE */
    sprintf(log_buf,
      "job requesting nodes that will never be available - spec = %s",
      spec_param);

    log_err(-1, __func__, log_buf);
    if (naji != NULL)
      release_node_allocation(naji);

    return(-1);
    }
#endif

  if (all_reqs.total_nodes > 0)
    {
    /* nodes no currently available */
    /* FAILURE */
    sprintf(log_buf,
      "job allocation request exceeds currently available cluster nodes, %d requested, %d available",
      num,
      num - all_reqs.total_nodes);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);

    if (EMsg != NULL)
      {
      snprintf(EMsg, MAXLINE, "%s", log_buf);
      }

    if (naji != NULL)
      release_node_allocation(naji);

    return(0);
    } /* END if (all_reqs.total_nodes > 0) */

  /* SUCCESS - spec is ok */
  if (LOGLEVEL >= 6)
    {
    sprintf(log_buf, "job allocation debug(3): returning %d requested", num);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);

    DBPRT(("%s\n", log_buf));
    }

  return(num);
  }  /* END node_spec() */




#ifdef GEOMETRY_REQUESTS
/**
 * get_bitmap
 *
 * @param pjob (I) - the job whose bitmap is be retrieved
 * @param ProcBMPtr (O) - the ptr to the string where the bitmap will be stored
 * @param ProcBMSize (I) - the size of the string ProcBMPtr points to
 * @return FAILURE if there is no specified bitmap or either pjob or ProcBMStrPtr are NULL
 * @return SUCCESS otherwise
 */
int get_bitmap(

  job  *pjob,        /* I */
  int   ProcBMSize,  /* I */
  char *ProcBMPtr)   /* O */

  {
  resource     *presc;
  resource_def *prd;

  char          LocalBM[MAX_BM];

  if ((pjob == NULL) ||
      (ProcBMPtr == NULL))
    {
    return(FAILURE);
    }

  LocalBM[0] = '\0';

  /* read the bitmap from the resource list */
  prd = find_resc_def(svr_resc_def,"procs_bitmap",svr_resc_size);
  presc = find_resc_entry(&pjob->ji_wattr[JOB_ATR_resource],prd);
  
  if ((presc != NULL) && 
      (presc->rs_value.at_flags & ATR_VFLAG_SET))
    {
    snprintf(LocalBM,sizeof(LocalBM),"%s",presc->rs_value.at_val.at_str);
    }
  else
    {
    /* fail if there was no bitmap given */

    return(FAILURE);
    }

  if (LocalBM[0] == '\0')
    {
    /* fail if there was no bitmap given */

    return(FAILURE);
    }
  else
    {
    snprintf(ProcBMPtr,sizeof(LocalBM),"%s",LocalBM);
    return(SUCCESS);
    }
  } /* end get_bitmap() */




/**
 * node_satisfies_request
 *
 * @param pnode (I) - the node to check for validity
 * @param ProcBMStr (I) - the bitmap of procs requested
 * @return TRUE - if the node satisfies the bitmap, FALSE otherwise
 * @return BM_ERROR if the bitmap isn't valid
 *
 * NOTE: must always be called by a thread already locking the pnode's mutex
 */
int node_satisfies_request(

  struct pbsnode *pnode,     /* I */
  char           *ProcBMStr) /* I */

  {
  int BMLen;
  int BMIndex;

  struct pbssubn *snp; 

  if (IS_VALID_STR(ProcBMStr) == FALSE)
    return(BM_ERROR);

  /* nodes are exclusive when we're using bitmaps */
  if (pnode->nd_state != INUSE_FREE)
    return(FALSE);

  BMLen = strlen(ProcBMStr);

  /* process in reverse because ProcBMStr[0] referes to core index 0 */
  BMIndex = BMLen-1;

  /* check if the requested processors are available on this node */
  for (snp = pnode->nd_psn;snp && BMIndex >= 0;snp = snp->next)
    {
    /* don't check cores that aren't requested */
    if (ProcBMStr[BMIndex--] != '1')
      continue;

    /* cannot use this node, one of the requested cores is busy */
    if (snp->inuse != INUSE_FREE)
      return(FALSE);
    }

  if (BMIndex >= 0)
    {
    /* this means we didn't finish checking the string -
     * the node doesn't have enough processors */

    return(FALSE);
    }

  /* passed all checks, we're good */
  return(TRUE);
  } /* END node_satisfies_request() */




/**
 * reserve_node
 *
 * @param pnode - node to reserve
 * @param pjob - the job to be added to the node
 * @param hlistptr - a pointer to the host list 
 */

int reserve_node(

  struct pbsnode  *pnode,     /* I/O */
  short            newstate,  /* I */
  job             *pjob,      /* I */
  char            *ProcBMStr, /* I */
  struct howl    **hlistptr)  /* O */

  {
  int             BMLen;
  int             BMIndex;

  struct pbssubn *snp; 

  if ((pnode == NULL) ||
      (pjob == NULL) ||
      (hlistptr == NULL))
    {
    return(FAILURE);
    }

  BMLen = strlen(ProcBMStr);
  BMIndex = BMLen-1;

  /* now reserve each node */
  for (snp = pnode->nd_psn;snp && BMIndex >= 0;snp = snp->next)
    {
    /* ignore unrequested cores */
    if (ProcBMStr[BMIndex--] != '1')
      continue;

    add_job_to_node(pnode,snp,INUSE_JOB,pjob,exclusive);

    build_host_list(hlistptr,snp,pnode);
    }
  
  /* mark the node as exclusive */
  pnode->nd_state = INUSE_JOB;

  return(SUCCESS);
  }
#endif /* GEOMETRY_REQUESTS */




/**
 * adds this job to the node's list of jobs
 * checks to be sure not to add duplicates
 *
 * conditionally updates the subnode's state
 * decrements the amount of needed nodes
 *
 * @param pnode - the node that the job is running on
 * @param nd_psn - the subnode (processor) that the job is running on
 * @param newstate - the state nodes are transitioning to when used
 * @param pjob - the job that is going to be run
 * @param exclusive - TRUE if jobs are given exclusive node use, FALSE otherwise
 */
int add_job_to_node(

  struct pbsnode *pnode,     /* I/O */
  struct pbssubn *snp,       /* I/O */
  short           newstate,  /* I */
  job            *pjob,      /* I */
  int             exclusive) /* I */

  {
  struct jobinfo *jp;
  char            log_buf[LOCAL_LOG_BUF_SIZE];

  /* NOTE:  search existing job array.  add job only if job not already in place */
  if (LOGLEVEL >= 5)
    {
    sprintf(log_buf, "allocated node %s/%d to job %s (nsnfree=%d)",
      pnode->nd_name,
      snp->index,
      pjob->ji_qs.ji_jobid,
      pnode->nd_nsnfree);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    DBPRT(("%s\n", log_buf));
    }

  for (jp = snp->jobs;jp != NULL;jp = jp->next)
    {
    if (!(strcmp(jp->jobid, pjob->ji_qs.ji_jobid)))
      break;
    }

  if (jp == NULL)
    {
    /* add job to front of subnode job array */
    jp = (struct jobinfo *)calloc(1, sizeof(struct jobinfo));

    jp->next = snp->jobs;
    snp->jobs = jp;
    strcpy(jp->jobid, pjob->ji_qs.ji_jobid);

    /* reduce free count */
    pnode->nd_nsnfree--;

    /* if no free VPs, set node state */
    if (pnode->nd_nsnfree <= 0)
      pnode->nd_state = newstate;

    if (snp->inuse == INUSE_FREE)
      {
      snp->inuse = newstate;

      if (!exclusive)
        pnode->nd_nsnshared++;
      }
    }

  /* decrement the amount of nodes needed */
  --pnode->nd_np_to_be_used;

  return(SUCCESS);
  } /* END add_job_to_node() */


    

int add_job_to_gpu_subnode(
    
  struct pbsnode *pnode,
  struct gpusubn *gn,
  job            *pjob)

  {
#ifdef NVIDIA_GPUS
  if (!pnode->nd_gpus_real)
#endif  /* NVIDIA_GPUS */
    {
    /* update the gpu subnode */
    strcpy(gn->jobid, pjob->ji_qs.ji_jobid);
    gn->inuse = TRUE;

    /* update the main node */
    pnode->nd_ngpus_free--;
    }

  gn->job_count++;
  pnode->nd_ngpus_to_be_used--;

  return(PBSE_NONE);
  } /* END add_job_to_gpu_subnode() */





/**
 * builds the host list (hlist)
 *
 * @param pnode - the node being added to the host list
 * @param hlist - the host list being built
 */ 
int build_host_list(

  struct howl    **hlistptr,  /* O */
  struct pbssubn  *snp,       /* I */
  struct pbsnode  *pnode)     /* I */
  
  {
  struct howl *curr;
  struct howl *prev;
  struct howl *hp;

  /* initialize the pointers */
  curr = (struct howl *)calloc(1, sizeof(struct howl));
  curr->order = pnode->nd_order;
  curr->name  = pnode->nd_name;
  curr->index = snp->index;
  curr->port = pnode->nd_mom_rm_port;

  /* find the proper place in the list */
  for (prev = NULL, hp = *hlistptr;hp;prev = hp, hp = hp->next)
    {
    if (curr->order <= hp->order)
      break;
    }  /* END for (prev) */

  /* set the correct pointers in the list */
  curr->next = hp;

  if (prev == NULL)
    *hlistptr = curr;
  else
    prev->next = curr;

  return(SUCCESS);
  }





int add_gpu_to_hostlist(
    
  struct howl    **hlistptr,
  struct gpusubn  *gn,
  struct pbsnode  *pnode)

  {
  struct howl *curr;
  struct howl *prev;
  struct howl *hp;
  char        *gpu_name;
  static char *gpu = "gpu";

  /* create gpu_name */
  gpu_name = calloc(1, strlen(pnode->nd_name) + strlen(gpu) + 2);
  sprintf(gpu_name, "%s-%s", pnode->nd_name, gpu);


  /* initialize the pointers */
  curr = (struct howl *)calloc(1, sizeof(struct howl));
  curr->order = pnode->nd_order;
  curr->name  = gpu_name;
  curr->index = gn->index;
  curr->port = pnode->nd_mom_rm_port;

  /* find the proper place in the list */
  for (prev = NULL, hp = *hlistptr;hp;prev = hp, hp = hp->next)
    {
    if (curr->order <= hp->order)
      break;
    }  /* END for (prev) */

  /* set the correct pointers in the list */
  curr->next = hp;

  if (prev == NULL)
    *hlistptr = curr;
  else
    prev->next = curr;

  return(SUCCESS);
  } /* END add_gpu_to_hostlist() */



/*
 * checks the gpus of pnode and places them in gpu_list as necessary
 */

int place_gpus_in_hostlist(

  struct pbsnode     *pnode,
  job                *pjob,
  node_job_add_info  *naji,
  struct howl       **gpu_list)

  {
  int             j;
  struct gpusubn *gn;

  char            log_buf[LOCAL_LOG_BUF_SIZE];

  /* place the gpus in the hostlist as well */
  for (j = 0; j < pnode->nd_ngpus && naji->gpu_needed > 0; j++)
    {
    sprintf(log_buf,
      "node: %s j %d ngpus %d need %d",
      pnode->nd_name,
      j,
      pnode->nd_ngpus,
      pnode->nd_ngpus_needed);
    
    if (LOGLEVEL >= 7)
      {
      log_ext(-1, __func__, log_buf, LOG_DEBUG);
      }
    DBPRT(("%s\n", log_buf));
    
    gn = pnode->nd_gpusn + j;
    if ((gn->state == gpu_unavailable) ||
#ifdef NVIDIA_GPUS
        ((gn->state == gpu_exclusive) && pnode->nd_gpus_real) ||
        ((pnode->nd_gpus_real) &&
         ((int)gn->mode == gpu_normal) &&
         ((gpu_mode_rqstd != gpu_normal) && (gn->state != gpu_unallocated))) ||
        ((!pnode->nd_gpus_real) && 
         (gn->inuse == TRUE)))
#else
      (gn->inuse == TRUE))
#endif  /* NVIDIA_GPUS */
        continue;
    
    add_job_to_gpu_subnode(pnode,gn,pjob);
    naji->gpu_needed--;
    
    sprintf(log_buf,
      "ADDING gpu %s/%d to exec_gpus still need %d",
      pnode->nd_name,
      j,
      pnode->nd_ngpus_needed);

    if (LOGLEVEL >= 7)
      {
      log_ext(-1, __func__, log_buf, LOG_DEBUG);
      }
    DBPRT(("%s\n", log_buf));
    
    add_gpu_to_hostlist(gpu_list,gn,pnode);
    
#ifdef NVIDIA_GPUS
    /*
     * If this a real gpu in exclusive/single job mode, or a gpu in default
     * mode and the job requested an exclusive mode then we change state
     * to exclusive so we cannot assign another job to it
     */
    
    if ((pnode->nd_gpus_real) && 
        ((gn->mode == gpu_exclusive_thread) ||
         (gn->mode == gpu_exclusive_process) ||
         ((gn->mode == gpu_normal) && 
          ((gpu_mode_rqstd == gpu_exclusive_thread) ||
           (gpu_mode_rqstd == gpu_exclusive_process)))))
      {
      gn->state = gpu_exclusive;
      
      sprintf(log_buf,
        "Setting gpu %s/%d to state EXCLUSIVE for job %s",
        pnode->nd_name,
        j,
        pjob->ji_qs.ji_jobid);
      
      if (LOGLEVEL >= 7)
        {
        log_ext(-1, __func__, log_buf, LOG_DEBUG);
        }
      }
    
    /*
     * If this a real gpu in shared/default job mode and the state is
     * unallocated then we change state to shared so only other shared jobs
     * can use it
     */
    
    if ((pnode->nd_gpus_real) && (gn->mode == gpu_normal) && 
        (gpu_mode_rqstd == gpu_normal) && (gn->state == gpu_unallocated))
      {
      gn->state = gpu_shared;
      
      sprintf(log_buf,
        "Setting gpu %s/%d to state SHARED for job %s",
        pnode->nd_name,
        j,
        pjob->ji_qs.ji_jobid);
      
      if (LOGLEVEL >= 7)
        {
        log_ext(-1, __func__, log_buf, LOG_DEBUG);
        }
      }
#endif  /* NVIDIA_GPUS */
    }

  return(PBSE_NONE);
  } /* END place_gpus_in_hostlist() */



/*
 * checks the subnodes of pnode and places them in the host list
 * as necessary
 */

int place_subnodes_in_hostlist(

  struct howl       **hlist,
  job                *pjob,
  short               newstate,
  struct pbsnode     *pnode,
  node_job_add_info  *naji)

  {
  struct pbssubn *snp;

  /* place the subnodes (nps) in the hostlist */
  for (snp = pnode->nd_psn; snp && naji->ppn_needed > 0; snp = snp->next)
    {
    if (exclusive)
      {
      if (snp->inuse != INUSE_FREE)
        continue;
      }
    else
      {
      if ((snp->inuse != INUSE_FREE) && (snp->inuse != INUSE_JOBSHARE))
        {
        continue;
        }
      }
    
    /* Mark subnode as being IN USE */
    add_job_to_node(pnode, snp, newstate, pjob, exclusive);
    build_host_list(hlist, snp, pnode);
    naji->ppn_needed--;
    }  /* END for (snp) */

  return(PBSE_NONE);
  } /* END place_subnodes_in_hostlist() */



/*
 * takes a struct howl and translates it to a string that will
 * become a job pbs_attribute (exec_hosts, exec_gpus, exec_ports)
 * NOTE: frees list (the struct howl)
 */

int translate_howl_to_string(

  struct howl  *list,
  char         *EMsg,
  int          *NCount,
  char        **str_ptr,
  char        **portstr_ptr,
  int           port)

  {
  struct howl *hp;
  struct howl *next;
  size_t       len = 1;
  int          count = 1;
  char        *str;
  char        *portlist = NULL;

  for (hp = list;hp != NULL;hp = hp->next)
    {
    len += (strlen(hp->name) + 6);
    count++;
    }

  if ((str = calloc(1, len + 1)) == NULL)
    {
    log_err(ENOMEM, __func__, "Cannot allocate memory!");

    if (EMsg != NULL)
      sprintf(EMsg,"no nodes can be allocated to job");
    
    return(PBSE_RESCUNAV);
    }

  *str = '\0';

  if (port == TRUE)
    {
    /* port list will have a string of sister port addresses */
    if ((portlist = calloc(1, (count * PBS_MAXPORTNUM) + count)) == NULL)
      {
      log_err(ENOMEM, __func__, "Cannot allocate memory!");
      
      if (EMsg != NULL)
        sprintf(EMsg,"no nodes can be allocated to job");

      free(str);
      
      return(PBSE_RESCUNAV);
      }
  
    *portlist = '\0';
    }

  /* now copy in name+name+... */
  *NCount = 0;

  for (hp = list; hp != NULL; hp = next)
    {
    (*NCount)++;

    sprintf(str + strlen(str), "%s/%d+",
      hp->name,
      hp->index);

    if (port == TRUE)
      sprintf(portlist + strlen(portlist), "%d+", hp->port);

    next = hp->next;

    free(hp);
    }

  /* strip trailing '+' and assign pointers */
  str[strlen(str) - 1] = '\0';
  *str_ptr = str;

  if (port == TRUE)
    {
    portlist[strlen(portlist) - 1] = '\0';
    *portstr_ptr = portlist;
    }

  return(PBSE_NONE);
  } /* END translate_howl_to_string() */




/*
 * free the struct that holds the information for where the job
 * will be placed  
 **/
void free_naji(
    
  node_job_add_info *naji)

  {
  node_job_add_info *current = NULL;
  node_job_add_info *tmp = NULL;
  node_job_add_info *first = naji;

  current = naji;
  while (current != NULL) 
    {
    tmp = current;
    current = current->next;
    free(tmp);
    if (current == first)
      break;
    }
  } /* END free_naji() */



/*
 * external nodes refers only to nodes outside of the cray
 * for jobs that also have cray compute nodes
 */

int record_external_node(

  job            *pjob,
  struct pbsnode *pnode)

  {
  char         *external_nodes;
  unsigned int  len;

  if (pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str == NULL)
    {
    pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str = strdup(pnode->nd_name);
    pjob->ji_wattr[JOB_ATR_external_nodes].at_flags |= ATR_VFLAG_SET;
    }
  else
    {
    len = strlen(pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str) + strlen(pnode->nd_name) + 2;
    external_nodes = calloc(1, len);

    snprintf(external_nodes, len, "%s+%s",
      pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str, pnode->nd_name);

    free(pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str);

    pjob->ji_wattr[JOB_ATR_external_nodes].at_val.at_str = external_nodes;
    }

  return(PBSE_NONE);
  } /* END record_external_node() */





/*
 * builds the hostlist based on the nodes=... part of the request
 */

int build_hostlist_nodes_req(
    
  job                *pjob,     /* M */
  char               *EMsg,     /* O */
  char               *spec,     /* I */
  short               newstate, /* I */
  struct howl       **hlist,    /* O */
  struct howl       **gpu_list, /* O */
  node_job_add_info  *naji)     /* I - freed */

  {
  struct pbsnode    *pnode = NULL;

  node_job_add_info *current;
  char               log_buf[LOCAL_LOG_BUF_SIZE];
  int                failure = FALSE;

  current = naji;

  while (current != NULL)
    {
    if ((pnode = find_nodebyname(current->node_name)) != NULL)
      {
      if (failure == TRUE)
        {
        /* just remove the marked request from the node */
        pnode->nd_np_to_be_used    -= current->ppn_needed;
        pnode->nd_ngpus_to_be_used -= current->gpu_needed;
        }
      else
        {
        place_gpus_in_hostlist(pnode, pjob, current, gpu_list);      
        place_subnodes_in_hostlist(hlist, pjob, newstate, pnode, current);
        
        if (current->is_external == TRUE)
          {
          record_external_node(pjob, pnode);
          }

        if ((naji->gpu_needed > 0) || 
            (naji->ppn_needed > 0))
          {
          failure = TRUE;
       
          /* remove any remaining things marked on the node */
          pnode->nd_np_to_be_used    -= current->ppn_needed;
          pnode->nd_ngpus_to_be_used -= current->gpu_needed;
          }
        }

      unlock_node(pnode, __func__, NULL, LOGLEVEL);
      }

    current = current->next;
    } /* END processing reserved nodes */
   
  free_naji(naji);

  if (failure == TRUE)
    {
    /* did not satisfy the request */
    if (EMsg != NULL)
      {
      sprintf(log_buf,
        "could not locate requested gpu resources '%.4000s' (node_spec failed) %s",
        spec,
        EMsg);
      
      log_record(PBSEVENT_JOB,PBS_EVENTCLASS_JOB,pjob->ji_qs.ji_jobid,log_buf);
      }

    return(PBSE_RESCUNAV);
    }

  return(PBSE_NONE);
  } /* END build_hostlist_nodes_req() */




int build_hostlist_procs_req(

  job          *pjob,     /* M */
  int           procs,    /* I */
  short         newstate, /* I */
  struct howl **hlist)    /* O */

  {
  int             procs_needed;
  node_iterator   iter;
  struct pbsnode *pnode = NULL;
  struct pbssubn *snp;

  /* did we have a request for procs? Do those now */
  if (procs > 0)
    {
    /* check to see if a -l nodes request was made */
    if (pjob->ji_have_nodes_request)
      {
      procs_needed = procs;
      }
    else
      {
      /* the qsub request used -l procs only. No -l nodes=x
         was given in the qsub request.
         TORQUE allocates 1 node by default if a -l nodes specification
         is not given.
      */
      if (procs > 1)
        {
        procs_needed = procs - 1;
        }
      else
        procs_needed = 1;
      }
  
    reinitialize_node_iterator(&iter);

    while ((pnode = next_node(&allnodes,pnode,&iter)) != NULL)
      {
      for (snp = pnode->nd_psn;snp && procs_needed > 0;snp = snp->next)
        {
        if (exclusive)
          {
          if (snp->inuse != INUSE_FREE)
            {
            continue;
            }
          }
        else
          {
          if ((snp->inuse != INUSE_FREE) && (snp->inuse != INUSE_JOBSHARE))
            {
            continue;
            }
          }

        /* Mark subnode as being IN USE */
        pnode->nd_needed++; /* we do this because add_job_to_node will decrement it */

        /* We need to set the node to thinking. */
        pnode->nd_flag = thinking;
        add_job_to_node(pnode,snp,newstate,pjob,exclusive);

        build_host_list(hlist,snp,pnode);
        procs_needed--;
        } /* END for (snp) */
      } /* END for each node */
    } /* if (procs > 0) */

  return(PBSE_NONE);
  } /* END build_hostlist_procs_req() */




int add_to_ms_list(
    
  char *node_name,
  job  *pjob)

  {
  struct pbsnode *pnode = find_nodebyname(node_name);

  if (pnode != NULL)
    {
    insert_thing(pnode->nd_ms_jobs, strdup(pjob->ji_qs.ji_jobid));

    unlock_node(pnode, __func__, NULL, 0);
    }

  return(PBSE_NONE);
  } /* END add_to_ms_list() */




void free_alps_req_data_array(
    
  alps_req_data *ard_array,
  int            num_reqs)

  {
  int i;

  for (i = 0; i < num_reqs; i++)
    free_dynamic_string(ard_array[i].node_list);

  free(ard_array);
  } /* END free_alps_req_data_array() */




int add_multi_reqs_to_job(
    
  job           *pjob,
  int            num_reqs,
  alps_req_data *ard_array)

  {
  int             i;
  dynamic_string *attr_str;
  char            buf[MAXLINE];

  if (ard_array == NULL)
    return(PBSE_NONE);

  attr_str = ard_array[0].node_list;

  for (i = 0; i < num_reqs; i++)
    {
    if (i != 0)
      {
      append_char_to_dynamic_string(attr_str, '|');
      append_dynamic_string(attr_str, ard_array[i].node_list->str);
      }

    snprintf(buf, sizeof(buf), "*%d", ard_array[i].ppn);
    append_dynamic_string(attr_str, buf);
    }

  if (pjob->ji_wattr[JOB_ATR_multi_req_alps].at_val.at_str != NULL)
    free(pjob->ji_wattr[JOB_ATR_multi_req_alps].at_val.at_str);

  pjob->ji_wattr[JOB_ATR_multi_req_alps].at_val.at_str = strdup(attr_str->str);
  pjob->ji_wattr[JOB_ATR_multi_req_alps].at_flags |= ATR_VFLAG_SET;

  return(PBSE_NONE);
  } /* END add_multi_reqs_to_job() */




/*
 * set_nodes() - Call node_spec() to allocate nodes then set them inuse.
 * Build list of allocated nodes to pass back in rtnlist.
 *      Return: PBS error code
 */

int set_nodes(

  job   *pjob,        /* I */
  char  *spec,        /* I */
  int    procs,       /* I */
  char **rtnlist,     /* O */
  char **rtnportlist, /* O */
  char  *FailHost,    /* O (optional,minsize=1024) */
  char  *EMsg)        /* O (optional,minsize=1024) */

  {
  struct howl       *hlist;
  struct howl       *gpu_list;

  int                i;
  int                rc;
  int                NCount;
  short              newstate;

  char              *login_prop = NULL;
  char              *gpu_str = NULL;
  char               ProcBMStr[MAX_BM];
  char               log_buf[LOCAL_LOG_BUF_SIZE];
  node_job_add_info *naji = NULL;
  alps_req_data     *ard_array = NULL;
  int                num_reqs = 0;
  long               cray_enabled = FALSE; 

#ifdef NVIDIA_GPUS
  int gpu_flags = 0;
#endif  /* NVIDIA_GPUS */

  if (FailHost != NULL)
    FailHost[0] = '\0';

  if (EMsg != NULL)
    EMsg[0] = '\0';

  if (LOGLEVEL >= 3)
    {
    sprintf(log_buf, "allocating nodes for job %s with node expression '%.4000s'",
      pjob->ji_qs.ji_jobid,
      spec);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  ProcBMStr[0] = '\0';
#ifdef GEOMETRY_REQUESTS
  get_bitmap(pjob,sizeof(ProcBMStr),ProcBMStr);
#endif /* GEOMETRY_REQUESTS */

  naji = calloc(1, sizeof(node_job_add_info));

  if (pjob->ji_wattr[JOB_ATR_login_prop].at_flags & ATR_VFLAG_SET)
    login_prop = pjob->ji_wattr[JOB_ATR_login_prop].at_val.at_str;

  /* allocate nodes */
  if ((i = node_spec(spec, 1, 1, ProcBMStr, FailHost, naji, EMsg, login_prop, &ard_array, &num_reqs)) == 0)
    {
    /* no resources located, request failed */
    if (EMsg != NULL)
      {
      sprintf(log_buf,
        "could not locate requested resources '%.4000s' (node_spec failed) %s",
        spec,
        EMsg);

      log_record(PBSEVENT_JOB,PBS_EVENTCLASS_JOB,pjob->ji_qs.ji_jobid,log_buf);
      }

    free_naji(naji);
    free_alps_req_data_array(ard_array, num_reqs);

    return(PBSE_RESCUNAV);
    }
  else if (i < 0)
    {
    /* request failed, corrupt request */
    log_err(PBSE_UNKNODE, __func__, "request failed, corrupt request");
    free_naji(naji);
    free_alps_req_data_array(ard_array, num_reqs);
    return(PBSE_UNKNODE);
    }

  hlist = NULL;
  gpu_list = NULL;

  newstate = exclusive ? INUSE_JOB : INUSE_JOBSHARE;

  if ((rc = build_hostlist_nodes_req(pjob, EMsg, spec, newstate, &hlist, &gpu_list, naji)) != PBSE_NONE)
    {
    free_alps_req_data_array(ard_array, num_reqs);
    return(rc);
    }

  if ((rc = build_hostlist_procs_req(pjob, procs, newstate, &hlist)) != PBSE_NONE)
    {
    free_alps_req_data_array(ard_array, num_reqs);
    return(rc);
    }

  if (hlist == NULL)
    {
    if (LOGLEVEL >= 1)
      {
      sprintf(log_buf, "no nodes can be allocated to job %s",
        pjob->ji_qs.ji_jobid);

      log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
      }

    if (EMsg != NULL)
      sprintf(EMsg, "no nodes can be allocated to job");
    
    free_alps_req_data_array(ard_array, num_reqs);

    return(PBSE_RESCUNAV);
    }  /* END if (hlist == NULL) */

  pjob->ji_qs.ji_svrflags |= JOB_SVFLG_HasNodes;  /* indicate has nodes */

  /* add this job to ms's job list --not used right now, will be debugged later */
  /*add_to_ms_list(hlist->name, pjob);*/

  /* build list of allocated nodes, gpus, and ports */
  if ((rc = translate_howl_to_string(hlist, EMsg, &NCount, rtnlist, rtnportlist, TRUE)) != PBSE_NONE)
    {
    free_alps_req_data_array(ard_array, num_reqs);
    return(rc);
    }

  get_svr_attr_l(SRV_ATR_CrayEnabled, &cray_enabled);
  if (cray_enabled == TRUE)
    {
    char *plus = strchr(*rtnlist, '+');
    char *login_name;

    /* only do this if there's more than one host in the host list */
    if (plus != NULL)
      {
      char *to_free = *rtnlist;

      *plus = '\0';
      login_name = strdup(*rtnlist);
      *rtnlist = strdup(plus + 1);
      free(to_free);
      

      pjob->ji_wattr[JOB_ATR_login_node_id].at_val.at_str = login_name;
      pjob->ji_wattr[JOB_ATR_login_node_id].at_flags = ATR_VFLAG_SET;
      }
    }

  if (gpu_list != NULL)
    {
    if ((rc = translate_howl_to_string(gpu_list, EMsg, &NCount, &gpu_str, NULL, FALSE)) != PBSE_NONE)
      {
      free_alps_req_data_array(ard_array, num_reqs);
      return(rc);
      }

    job_attr_def[JOB_ATR_exec_gpus].at_free(
      &pjob->ji_wattr[JOB_ATR_exec_gpus]);
    
    job_attr_def[JOB_ATR_exec_gpus].at_decode(
      &pjob->ji_wattr[JOB_ATR_exec_gpus],
      NULL,
      NULL,
      gpu_str,
      0);  /* O */
    
    free(gpu_str);

#ifdef NVIDIA_GPUS
    if (gpu_mode_rqstd != -1)
      gpu_flags = gpu_mode_rqstd;
    if (gpu_err_reset)
      gpu_flags += 1000;

    if (gpu_flags >= 0)
      {
      pjob->ji_wattr[JOB_ATR_gpu_flags].at_val.at_long = gpu_flags;
      pjob->ji_wattr[JOB_ATR_gpu_flags].at_flags = ATR_VFLAG_SET | ATR_VFLAG_MODIFY;
      
      if (LOGLEVEL >= 7)
        {
        sprintf(log_buf, "setting gpu_flags for job %s to %d %ld",
          pjob->ji_qs.ji_jobid,
          gpu_flags,
          pjob->ji_wattr[JOB_ATR_gpu_flags].at_val.at_long);

        log_ext(-1, __func__, log_buf, LOG_DEBUG);
        }
      }
#endif  /* NVIDIA_GPUS */
    }

  if (LOGLEVEL >= 3)
    {
    snprintf(log_buf, sizeof(log_buf), "job %s allocated %d nodes (nodelist=%.4000s)",
      pjob->ji_qs.ji_jobid,
      NCount,
      *rtnlist);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  add_multi_reqs_to_job(pjob, num_reqs, ard_array);
  free_alps_req_data_array(ard_array, num_reqs);

  /* SUCCESS */

  return(PBSE_NONE);
  }  /* END set_nodes() */




/* count the number of requested processors in a node spec
 * return processors requested on success
 * return -1 on error 
 */ 
int procs_requested(
    
  char *spec)

  {
  char        *str;
  char        *globs;
  char        *cp;
  char        *hold;
  int          num_nodes = 0;
  int          num_procs = 0;
  int          total_procs = 0;
  int          num_gpus = 0;
  int          i;
  struct prop *prop = NULL;
  char        *tmp_spec;
  char         log_buf[LOCAL_LOG_BUF_SIZE];

  tmp_spec = strdup(spec);  
  
  if (tmp_spec == NULL)
    {
    /* FAILURE */

    sprintf(log_buf,"cannot alloc memory");

    if (LOGLEVEL >= 1)
      {
      log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
      }

    return(-1);
    }

  /* Check to see if we have a global modifier */
  if ((globs = strchr(tmp_spec, '#')) != NULL)
    {
    *globs++ = '\0';

    globs = strdup(globs);

    while ((cp = strrchr(globs, '#')) != NULL)
      {
      *cp++ = '\0';

      if (strcmp(cp, shared) != 0)
        {
        hold = mod_spec(spec, cp);

        free(tmp_spec);

        tmp_spec = hold;
        }
      else
        {
        exclusive = 0;
        }
      }

    if (strcmp(globs, shared) != 0)
      {
      hold = mod_spec(tmp_spec, globs);

      free(tmp_spec);

      tmp_spec = hold;
      }
    else
      {
      exclusive = 0;
      }

    free(globs);
    }  /* END if ((globs = strchr(spec,'#')) != NULL) */

  str = tmp_spec;

  do
    {
    if ((i = number(&str, &num_nodes)) == -1 )
      {
      free(tmp_spec);
      /* Bad string syntax. Fail */
      return(-1);
      }

    if (i == 0)
      {
      /* number exists */
      if (*str == ':')
        {
        /* there are properties */

        str++;

        if (proplist(&str, &prop, &num_procs, &num_gpus))
          {
          free(tmp_spec);
          return(-1);
          }
        }
      }
    else
      {
      /* no number */
      num_nodes = 1;
      if (proplist(&str, &prop, &num_procs, &num_gpus))
        {
        /* must be a prop list with no number in front */
        free(tmp_spec);

        return(-1);
        }
      }
    total_procs += num_procs * num_nodes;
    } while(*str++ == '+');
  
  free(tmp_spec);
  
  return(total_procs);
  } /* END procs_requested() */





/*
 * node_avail_complex -
 * *navail is set to number available
 * *nalloc is set to number allocated
 * *nresvd is set to number reserved
 * *ndown  is set to number down/offline
 *      return -1 on failure
 */

int node_avail_complex(

  char *spec,   /* I - node spec */
  int  *navail, /* O - number available */
  int  *nalloc, /* O - number allocated */
  int  *nresvd, /* O - number reserved  */
  int  *ndown)  /* O - number down      */

  {
  int ret;

  ret = node_spec(spec, 1, 0, NULL, NULL, NULL, NULL, NULL, NULL, NULL);

  *navail = ret;
  *nalloc = 0;
  *nresvd = 0;
  *ndown  = 0;

  return(ret);
  }  /* END node_avail_complex() */





/*
 * node_avail - report if nodes requested are available
 * Does NOT even consider Time Shared Nodes
 *
 * Return 0 when no error in request and
 *  *navail is set to number available
 *  *nalloc is set to number allocated
 *  *nresvd is set to number reserved
 *  *ndown  is set to number down/offline
 *      !=0 error number when error in request
 */

int node_avail(

  char *spec,  /* I  - node spec */
  int  *navail, /* O - number available */
  int *nalloc, /* O - number allocated */
  int *nresvd, /* O - number reserved  */
  int *ndown)  /* O - number down      */

  {
  int             j;
  int             holdnum;

  struct pbsnode *pn;
  struct pbssubn *psn;
  char           *pc;

  struct prop    *prop = NULL;
  register int    xavail;
  register int    xalloc;
  register int    xresvd;
  register int    xdown;
  int             node_req = 1;
  int             gpu_req = 0;

  node_iterator   iter;

  if (spec == NULL)
    {
    log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, "no spec");

    return(RM_ERR_NOPARAM);
    }

  pc = spec;

  if ((strchr(spec, (int)'+') == NULL) && (number(&pc, &holdnum) == 1))
    {
    /* A simple node spec - reply with numbers of avaiable, */
    /* allocated, reserved, and down nodes that match the */
    /* the spec, null or simple number means all  */

    xavail = 0;
    xalloc = 0;
    xresvd = 0;
    xdown  = 0;

    /* find number of a specific type of node */

    if (*pc)
      {
      if (proplist(&pc, &prop, &node_req, &gpu_req))
        {
        return(RM_ERR_BADPARAM);
        }
      }

    reinitialize_node_iterator(&iter);
    pn = NULL;

    while ((pn = next_node(&allnodes, pn, &iter)) != NULL)
      {
      if ((pn->nd_ntype == NTYPE_CLUSTER) && hasprop(pn, prop))
        {
        if (pn->nd_state & (INUSE_OFFLINE | INUSE_DOWN))
          ++xdown;
        else if (hasppn(pn, node_req, SKIP_ANYINUSE))
          ++xavail;
        else if (hasppn(pn, node_req, SKIP_NONE))
          {
          /* node has enough processors, are they busy or reserved? */
          j = 0;
          
          for (psn = pn->nd_psn;psn;psn = psn->next)
            {
            if (psn->inuse & INUSE_RESERVE)
              j++;
            }
          
          if (j >= node_req)
            ++xresvd;
          else
            ++xalloc;
          }
        }
      } /* END for each node */

    free_prop(prop);

    *navail = xavail;

    *nalloc = xalloc;

    *nresvd = xresvd;

    *ndown  = xdown;

    return(0);
    }
  else if (number(&pc, &holdnum) == -1)
    {
    /* invalid spec */

    return(RM_ERR_BADPARAM);
    }

  /* not a simple spec - determine if supplied complex */
  /* node spec can be satisified from avail nodes */
  /* navail set to >0 if can be satified now  */
  /*    0 if not now but possible  */
  /*   -l if never possible   */

  node_avail_complex(spec, navail, nalloc, nresvd, ndown);

  return(0);
  }  /* END node_avail() */




/*
 * node_reserve - Reserve nodes
 * Cannot reserve Time Shared Nodes
 *
 * Returns: >0 - reservation succeeded, number of nodes reserved
 *    0 - None or partial reservation
 *   -1 - requested reservation impossible
 */

int node_reserve(

  char       *nspec, /* In     - a node specification */
  resource_t  tag)   /* In/Out - tag for resource if reserved */

  {
  int                nrd;

  struct pbsnode    *pnode;
  struct pbssubn    *snp;
  int                ret_val;

  node_iterator      iter;
  char               log_buf[LOCAL_LOG_BUF_SIZE];
  node_job_add_info  *naji = NULL;

  DBPRT(("%s: entered\n", __func__))

  if ((nspec == NULL) || (*nspec == '\0'))
    {
    log_event(PBSEVENT_ADMIN, PBS_EVENTCLASS_SERVER, __func__, "no spec");

    return(-1);
    }

  naji = calloc(1, sizeof(node_job_add_info));

  if ((ret_val = node_spec(nspec, 0, 0, NULL, NULL, naji, NULL, NULL, NULL, NULL)) >= 0)
    {
    /*
    ** Zero or more of the needed Nodes are available to be
    ** reserved.
    */
    reinitialize_node_iterator(&iter);
    pnode = NULL;

    while ((pnode = next_node(&allnodes,pnode,&iter)) != NULL)
      {
      if (pnode->nd_flag != thinking)
        {
        continue;   /* skip this one */
        }

      nrd = 0;

      for (snp = pnode->nd_psn;snp && pnode->nd_needed;snp = snp->next)
        {
        if (snp->inuse == INUSE_FREE)
          {
          DBPRT(("hold %s/%d\n",
                 pnode->nd_name,
                 snp->index))

          snp->inuse |= INUSE_RESERVE;
          snp->allocto = tag;

          pnode->nd_nsnfree--;  /* in reserve, not reached? */

          --pnode->nd_needed;

          ++nrd;
          }
        }

      if (nrd == pnode->nd_nsn)
        pnode->nd_state = INUSE_RESERVE;
      } /* END for each node */
    }
  else
    {
    /* could never satisfy the reservation */

    snprintf(log_buf, sizeof(log_buf), "can never reserve %s", nspec);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  free_naji(naji);

  return(ret_val);
  }  /* END node_reserve() */






/*
 * is_ts_node - does the nodestr specify a single time share node?
 * 0 - yes
 * 1 - no, not a ts node or more than one node (name will not match)
 */

int is_ts_node(

  char *nodestr)

  {
  int             rc = 1; /* see comment above */
  struct pbsnode *np = find_nodebyname(nodestr);

  if (np != NULL)
    {
    if (np->nd_ntype == NTYPE_TIMESHARED)
      {
      rc = 0;
      }

    unlock_node(np, "is_ts_node", NULL, LOGLEVEL);
    }

  return(rc);
  }  /* END is_ts_node() */





/*
 * find_ts_node - find first up time-shared node
 *
 * returns name of node or null
 */

char *find_ts_node(void)

  {
  struct pbsnode *np = NULL;
  node_iterator   iter;

  reinitialize_node_iterator(&iter);

  while ((np = next_node(&allnodes,np,&iter)) != NULL)
    {
    if ((np->nd_ntype == NTYPE_TIMESHARED) &&
        ((np->nd_state & (INUSE_DOWN | INUSE_OFFLINE)) == 0))
      {
      char *name = np->nd_name;

      unlock_node(np, "find_ts_node", NULL, LOGLEVEL);

      return(name);
      }
    } /* END for each node */

  return(NULL);
  }  /* END find_ts_node() */



char *get_next_exec_host(

  char **current)

  {
  char *name_ptr = *current;
  char *plus;
  char *slash;
  
  if (name_ptr != NULL)
    {
    if ((plus = strchr(name_ptr, '+')) != NULL)
      {
      *current = plus + 1;
      *plus = '\0';
      }
    else
      *current = NULL;

    if ((slash = strchr(name_ptr, '/')) != NULL)
      *slash = '\0';
    }

  return(name_ptr);
  } /* END get_next_exec_host() */



int remove_job_from_nodes_gpus(

  struct pbsnode *pnode,
  job            *pjob)

  {
  struct gpusubn *gn;
  char           *gpu_str = NULL;
  int             i;
#ifdef NVIDIA_GPUS
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  char            tmp_str[PBS_MAXHOSTNAME + 10];
  char            num_str[6];
#endif
 
  if (pjob->ji_wattr[JOB_ATR_exec_gpus].at_flags & ATR_VFLAG_SET)
    gpu_str = pjob->ji_wattr[JOB_ATR_exec_gpus].at_val.at_str;

  if (gpu_str != NULL)
    {
    /* reset gpu nodes */
    for (i = 0; i < pnode->nd_ngpus; i++)
      {
      gn = pnode->nd_gpusn + i;
#ifdef NVIDIA_GPUS
      if (pnode->nd_gpus_real)
        {
        /* reset real gpu nodes */
        strcpy (tmp_str, pnode->nd_name);
        strcat (tmp_str, "-gpu/");
        sprintf (num_str, "%d", i);
        strcat (tmp_str, num_str);
        
        /* look thru the string and see if it has this host and gpuid.
         * exec_gpus string should be in format of 
         * <hostname>-gpu/<index>[+<hostname>-gpu/<index>...]
         *
         * if we are using the gpu node exclusively or if shared mode and
         * this is last job assigned to this gpu then set it's state
         * unallocated so its available for a new job. Takes time to get the
         * gpu status report from the moms.
         */
        
        if (strstr(gpu_str, tmp_str) != NULL)
          {
          gn->job_count--;
          
          if ((gn->mode == gpu_exclusive_thread) ||
              (gn->mode == gpu_exclusive_process) ||
              ((gn->mode == gpu_normal) && 
               (gn->job_count == 0)))
            {
            gn->state = gpu_unallocated;
            
            if (LOGLEVEL >= 7)
              {
              sprintf(log_buf, "freeing node %s gpu %d for job %s",
                pnode->nd_name,
                i,
                pjob->ji_qs.ji_jobid);
              
              log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
              }
            
            }
          }
        }
      else
#endif  /* NVIDIA_GPUS */
        {
        if (!strcmp(gn->jobid, pjob->ji_qs.ji_jobid))
          {
          gn->inuse = FALSE;
          memset(gn->jobid, 0, sizeof(gn->jobid));
          
          pnode->nd_ngpus_free++;
          }
        }
      }
    }

  return(PBSE_NONE);
  } /* END remove_job_from_nodes_gpus() */




int remove_job_from_node(

  struct pbsnode *pnode,
  job            *pjob)

  {
  struct pbssubn *np;
  struct jobinfo *jp;
  struct jobinfo *prev = NULL;
  char            log_buf[LOCAL_LOG_BUF_SIZE];
  
  /* examine all subnodes in node */
  for (np = pnode->nd_psn;np != NULL;np = np->next)
    {
    /* examine all jobs allocated to subnode */
    
    for (prev = NULL, jp = np->jobs;jp != NULL;prev = jp, jp = jp->next)
      {
      if (strcmp(jp->jobid, pjob->ji_qs.ji_jobid))
        continue;
      
      if (LOGLEVEL >= 4)
        {
        sprintf(log_buf, "freeing node %s/%d from job %s (nsnfree=%d)",
          pnode->nd_name,
          np->index,
          pjob->ji_qs.ji_jobid,
          pnode->nd_nsnfree);
        
        log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
        }
      
      if (prev == NULL)
        np->jobs = jp->next;
      else
        prev->next = jp->next;

      free(jp);
      
      pnode->nd_nsnfree++; /* up count of free */
      
      if (LOGLEVEL >= 6)
        {
        sprintf(log_buf, "increased sub-node free count to %d of %d\n",
          pnode->nd_nsnfree,
          pnode->nd_nsn);
        
        log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
        }
      
      pnode->nd_state &= ~(INUSE_JOB | INUSE_JOBSHARE);
      
      /* if no jobs are associated with subnode, mark subnode as free */
      
      if (np->jobs == NULL)
        {
        if (np->inuse & INUSE_JOBSHARE)
          pnode->nd_nsnshared--;
        
        /* adjust node state (turn off job/job-exclusive) */        
        np->inuse &= ~(INUSE_JOB | INUSE_JOBSHARE);
        }
      
      break;
      }  /* END for (prev) */
    }    /* END for (np) */

  return(PBSE_NONE);
  } /* END remove_job_from_node() */




/*
 * free_nodes - free nodes allocated to a job
 */

void free_nodes(

  job *pjob)  /* I (modified) */

  {
  struct pbsnode *pnode;

  char            log_buf[LOCAL_LOG_BUF_SIZE];
  char           *exec_hosts = NULL;
  char           *host_ptr = NULL;
  char           *hostname;
  char           *previous_hostname = NULL;

  if (LOGLEVEL >= 3)
    {
    sprintf(log_buf, "freeing nodes for job %s", pjob->ji_qs.ji_jobid);

    log_record(PBSEVENT_SCHED, PBS_EVENTCLASS_REQUEST, __func__, log_buf);
    }

  if (pjob->ji_wattr[JOB_ATR_exec_host].at_flags & ATR_VFLAG_SET)
    {
    if (pjob->ji_wattr[JOB_ATR_exec_host].at_val.at_str != NULL)
      {
      exec_hosts = strdup(pjob->ji_wattr[JOB_ATR_exec_host].at_val.at_str);
      host_ptr = exec_hosts;
      }
    }

  while ((hostname = get_next_exec_host(&host_ptr)) != NULL)
    {
    if ((previous_hostname) != NULL)
      {
      if (!strcmp(hostname, previous_hostname))
        continue;
      }

    previous_hostname = hostname;

    if ((pnode = find_nodebyname(hostname)) != NULL)
      {
      remove_job_from_node(pnode, pjob);
      remove_job_from_nodes_gpus(pnode, pjob);
      unlock_node(pnode, __func__, NULL, 0);
      }
    }

  free(exec_hosts);

  if (pjob->ji_wattr[JOB_ATR_login_node_id].at_val.at_str != NULL)
    {
    if ((pnode = find_nodebyname(pjob->ji_wattr[JOB_ATR_login_node_id].at_val.at_str)) != NULL)
      {
      remove_job_from_node(pnode, pjob);
      unlock_node(pnode, __func__, NULL, 0);
      }
    }

  pjob->ji_qs.ji_svrflags &= ~JOB_SVFLG_HasNodes;

  return;
  }  /* END free_nodes() */




struct pbsnode *get_compute_node(

  char *node_name)

  {
  struct pbsnode *ar = alps_reporter;
  struct pbsnode *compute_node = NULL;
  unsigned int    i;
  unsigned int    len = strlen(node_name);

  for (i = 0; i < len; i++)
    {
    if (isdigit(node_name[i]) == FALSE)
      {
      /* found a non-numeric character - not a compute node */
      return(NULL);
      }
    }

  lock_node(ar, __func__, NULL, 0);
  compute_node = create_alps_subnode(ar, node_name);
  unlock_node(ar, __func__, NULL, 0);

  return(compute_node);
  } /* END get_compute_node() */




/*
 * set_one_old - set a named node as allocated to a job
 */

void set_one_old(

  char *name,
  job  *pjob,
  int   is_shared) /* how used flag, either INUSE_JOB or INUSE_JOBSHARE */

  {
  int             index;

  struct pbsnode *pnode;
  struct pbssubn *snp;

  struct jobinfo *jp;
  char           *pc;
  long            cray_enabled = FALSE;

  if ((pc = strchr(name, (int)'/')))
    {
    index = atoi(pc + 1);

    *pc = '\0';
    }
  else
    {
    index = 0;
    }

  get_svr_attr_l(SRV_ATR_CrayEnabled, &cray_enabled);

  pnode = find_nodebyname(name);

  if (cray_enabled == TRUE)
    {
    if (pnode == NULL)
      pnode = get_compute_node(name);

    if (pnode != NULL)
      {
      if (pnode->parent == alps_reporter)
        {
        while (index >= pnode->nd_nsn)
          {
          create_subnode(pnode);
          }
        }
      }
    }

  if (pnode != NULL)
    {
    /* Mark node as being IN USE ...  */
    if (pnode->nd_ntype == NTYPE_CLUSTER)
      {
      for (snp = pnode->nd_psn;snp;snp = snp->next)
        {
        if (snp->index == index)
          {
          snp->inuse = is_shared;
          
          jp = (struct jobinfo *)calloc(1, sizeof(struct jobinfo));
          
          /* NOTE:  should report failure if jp == NULL (NYI) */
          if (jp != NULL)
            {
            jp->next = snp->jobs;
            
            snp->jobs = jp;
            
            strcpy(jp->jobid, pjob->ji_qs.ji_jobid);
            }
          
          if (--pnode->nd_nsnfree <= 0)
            pnode->nd_state |= is_shared;
         
          break;
          }
        }    /* END for (snp) */
      }

    unlock_node(pnode, __func__, NULL, LOGLEVEL);
    }

  return;
  }  /* END set_one_old() */





/*
 * set_old_nodes - set "old" nodes as in use - called from pbsd_init()
 * when recovering a job in the running state.
 */

void set_old_nodes(

  job *pjob)  /* I (modified) */

  {
  char     *old;
  char     *po;
  resource *presc;
  int       is_shared = INUSE_JOB;

  if (pjob->ji_wattr[JOB_ATR_exec_host].at_flags & ATR_VFLAG_SET)
    {
    /* are the nodes being used shared? Look in "neednodes" */

    presc = find_resc_entry(
              &pjob->ji_wattr[JOB_ATR_resource],
              find_resc_def(svr_resc_def, "neednodes", svr_resc_size));

    if ((presc != NULL) && (presc->rs_value.at_flags & ATR_VFLAG_SET))
      {
      if ((po = strchr(presc->rs_value.at_val.at_str, '#')))
        {
        if (strstr(++po, shared) != NULL)
          is_shared = INUSE_JOBSHARE;
        }
      }

    old = strdup(pjob->ji_wattr[JOB_ATR_exec_host].at_val.at_str);

    if (old == NULL)
      {
      /* FAILURE - cannot alloc memory */

      return;
      }

    while ((po = strrchr(old, (int)'+')) != NULL)
      {
      *po++ = '\0';

      set_one_old(po, pjob, is_shared);
      }

    set_one_old(old, pjob, is_shared);

    free(old);
    }  /* END if pjobs exec host is set */

  return;
  }  /* END set_old_nodes() */

  
job *get_job_from_jobinfo(
    
  struct jobinfo *jp,
  struct pbsnode *pnode)
  
  {
  job *pjob;

  unlock_node(pnode, __func__, NULL, LOGLEVEL);
  pjob = svr_find_job(jp->jobid, TRUE);
  lock_node(pnode, __func__, NULL, LOGLEVEL);

  return(pjob);
  } /* END get_job_from_jobinfo() */


/* END node_manager.c */

